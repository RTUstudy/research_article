Page 3:
Speech Communication 154 (2023) 102974  3 A. Hashem et al.  Fig. 1.   Traditional machine learning flow mechanisms start with preprocessing and then extracting features to reduce the number of features; the features selection method is used and then goes as an input to the machine learning, where it gives the emotional state as an output (a), and deep learning flow mechanisms start with input audio, where based on the deep learning algorithm, it extracts and selects features and then makes the classification automatically to give the emotional state as an output (b).  Fig. 2.   The filtering process of selected articles by the three databases starts with 350 papers, and after applying exclusion criteria, filtering, and quality assessment, the number of papers was reduced to 96.  2.2.1. Search keywords  The following criteria were used to select the search keywords: 1. The specified new terms were based on books and published papers. 2. The search results were constrained using boolean operators (ANDs and ORs). The search Keywords that were utilized contain the following:  •   ‘‘Deep neural network’’ AND ‘‘speech emotion recognition’’ OR ‘‘Audio emotion recognition’’.  •   ‘‘Emotional speech database’’ AND ‘‘speech emotion recognition’’ OR ‘‘Emotion recognition from speech using deep learning ap- proaches’’.  •   ‘‘speech emotion Detection’’ OR ‘‘Speech features for emotion recognition from speech’’.  2.2.2. Digital database libraries  The digital libraries used to find the needed research papers are the following:  •   Google Scholar  •   IEEE Explorer  •   Science Direct. Table 2 shows the resource distribution of the selected research pa- pers. A total of 96 papers from 350 papers were included in this study. The SLR proposed by Kitchenham and Charters (Keele et al., 2007) inspired this systematic literature review. All articles were examined for inclusion in the study using the inclusion, exclusion, and quality evaluation criteria.  2.3. Search process  The earlier search keywords were used to extract the research papers from the selected digital libraries. Below are the study selection, inclusion/exclusion criteria, and quality assessment.  2.3.1. Study selection  The papers obtained were based on the search utilizing the listed search keywords. The following list contains the steps used for selection and filtration:  •   Step 1: Eliminate all duplicate research papers that were found in different digital libraries.  •   Step 2: Apply inclusion/exclusion criteria to confirm that this review contains only relevant papers.  •   Step 3: Remove reviews/surveys/systematic papers from the list of papers.

Page 13:
Speech Communication 154 (2023) 102974  13 A. Hashem et al.  Fig. 8.   The extracted speech feature categories used for speech emotion recognition.  Fig. 9.   The algorithm used to extract Mel Frequency Cepstral Coefficients feature.  where the speech signal represents the short-term power spectrum. Lin- ear Prediction Cepstral Coefficients (LPCC) and Gammatone Frequency Cepstral Coefficients (GFCC) are also spectral features (Sugan et al., 2018). Extracted MFCC features can be done throw several steps. As shown in Fig. 9, after the preprocessing, framing, and windowing covered in Section 5, a Fast Fourier Transform (FFT) is performed. FFT algorithm is the best choice for determining the speaker’s frequency spectrum (Chen and Huang, 2021; Pepino et al., 2021). FFT transforms each frame of samples from the time domain to the frequency domain. Then Mel filters bank and frequency wrapping: The cutoff frequencies of the triangle filters that make up the Mel filter bank (Lin and Wei, 2005) are defined based on the two neighboring filter centers. The filters have fixed bandwidth on the Mel scale and linearly spaced center frequencies. Taking logarithms, It can convert multiplication into addition. Thus, this process only transforms the Fourier transform’s multiplication of the magnitude into addition. The last step is Discrete Cosine Transform (DCT), which orthogonalizes the filter energy vectors. By performing orthogonalization, the energy vector of the filter is condensed into the initial set of components, resulting in a shortened vector comprising only a few components (Chavhan et al., 2010).  6.3. Voice quality features  The physical characteristics of the vocal tract determine voice qual- ity, which defines the qualities of the glottal source. As discussed by Cowie et al. (2001), they found a strong correlation between the emotional state of the speech and the voice quality. Voice quality in terms of the temporal structure period of the speech analysis is shorter than 10 ms; therefore, it is called subsegmental level features. Jitter, shimmer, and harmonics-to-noise ratio (HNR) are features used to detect voice quality. Jitter refers to the frequency difference between cycles, whereas shimmer refers to the amplitude difference of the sound wave (Teixeira et al., 2013). HNR is focused on measuring the ratio of speech sounds between periodic to non-periodic components, where it determines the correlation in the vocal tract between noise (non- periodic) and harmonics (periodic); the lower the HNR, the more noise in the voice (Fernandes et al., 2018).  6.4. Teager energy operator (TEO)  In the context of SER applications, airflow is assumed to be transmit- ted as a plane wave through the vocal tract and frequently use linear

Page 1:
Speech Communication 154 (2023) 102974  Available online 7 September 2023 0167-6393/© 2023 Elsevier B.V. All rights reserved. Contents lists available at ScienceDirect  Speech Communication  journal homepage: www.elsevier.com/locate/specom  Speech emotion recognition approaches: A systematic review  Ahlam Hashem, Muhammad Arif   ∗ , Manal Alghamdi  Department of Computer Science, Umm Al-Qura university, Makkah, Saudi Arabia  A R T I C L E   I N F O  Keywords:  Speech emotion recognition Emotional speech database Classification of emotion Speech features Systematic review  A B S T R A C T  The speech emotion recognition (SER) field has been active since it became a crucial feature in advanced Human–Computer Interaction (HCI), and wide real-life applications use it. In recent years, numerous SER systems have been covered by researchers, including the availability of appropriate emotional databases, selecting robustness features, and applying suitable classifiers using Machine Learning (ML) and Deep Learning (DL). Deep models proved to perform more accurately for SER than conventional ML techniques. Nevertheless, SER is yet challenging for classification where to separate similar emotional patterns; it needs a highly discriminative feature representation. For this purpose, this survey aims to critically analyze what is being done in this field of research in light of previous studies that aim to recognize emotions using speech audio in different aspects and review the current state of SER using DL. Through a systematic literature review whereby searching selected keywords from 2012–2022, 96 papers were extracted and covered the most current findings and directions. Specifically, we covered the database (acted, evoked, and natural) and features (prosodic, spectral, voice quality, and teager energy operator), the necessary preprocessing steps. Furthermore, different DL models and their performance are examined in depth. Based on our review, we also suggested SER aspects that could be considered in the future.  1. Introduction  Human emotions are a vital component of interaction in real-world communication, where they can express their emotions in many ways, including gestures, facial expressions, bodily postures, and speech. In addition, some physical characteristics can also give a clue to identi- fying human emotions, for example, heart rate, blood pressure, tem- perature, muscle activity, and skin resistance (Gangamohan et al., 2016). However, human speech is the best and easiest way to reflect emotions, where the essential research question is determining the emotional state of a particular speaker’s speech signal. The three critical components for identifying emotions in speech are ‘‘what is said’’, or the contents, ‘‘how it is said’’, or speaking manner, and ‘‘who is speaking it’’ (i.e., male or female) Therefore, much research aimed at facilitating communication between computers and humans tended to develop this field by making the computer do specific tasks through the direct human voice, known as speech recognition, and by using human emotion, known as automatic speech emotion recognition(SER). The increased number of studies done on this topic is significant, and this is because automatic emotion recognition can be used in applications for real-life issues from different fields. The acquisition and analysis of emotional states through recordings are significant in monitoring and treating mental illnesses. Gaining awareness of a  ∗   Corresponding author.  E-mail addresses:   A7lam7x@hotmail.com (A. Hashem), mahamid@uqu.edu.sa (M. Arif), maalghamdi@uqu.edu.sa (M. Alghamdi).  mental health patient’s emotions can offer valuable insights into devis- ing effective care strategies. However, manual recognition of patient emotions poses challenges for mental health practitioners (Copeland, 2002). Emotion recognition is a crucial aspect of mental healthcare and a core skill for any mental health practitioner (Minardi, 2013). Studies like the one conducted by Madanian et al. (2022) have explored how SER can be utilized for mental health assessment, emotion-aware therapy, and assistive technologies for individuals with communication disorders. In customer service, Han et al. (2020) have investigated real-time customer emotion recognition to analyze customer sentiments during interactions, helping companies enhance service quality and customer experience. In the education domain, Abdelhamid (2023) have highlighted the potential of SER to detect students’ emotional states, engagement levels, and attention during online learning or classroom interactions, thus facilitating the development of adaptive learning systems and improve student performance. SER has expanded its scope to include various applications in the entertainment industry. Many businesses leverage recommendation systems, particularly in e- commerce, to enhance user experience and engagement. For instance, emotion-based recommendations, such as selling books, movies, and music, have become increasingly popular. Bongirwar and Potnurwar (2022) proposed a song recommendation system that integrates emo- tion recognition as a vital parameter. Considering the user’s mood, this  https://doi.org/10.1016/j.specom.2023.102974 Received 13 April 2023; Received in revised form 10 August 2023; Accepted 22 August 2023

Page 2:
Speech Communication 154 (2023) 102974  2 A. Hashem et al.  system suggests songs that align with their emotional state, contribut- ing to a more personalized and enjoyable music experience. Driver safety has also been addressed through SER, where Requardt et al. (2020) aimed to explore SER to identify drivers’ emotions, including frustration, anxiety, and positive emotions, and distinguish them from a neutral emotional state. This investigation aimed to assess the sub- stantial impact of recognizing the driver’s emotional state on car safety, detect the driver’s mental state and reduce the likelihood of accidents. Combining SER and real-life applications has enhanced connections between humans and computers. By enabling systems to recognize and respond to human emotions, SER has opened new possibilities for more intuitive and emotionally responsive human–computer inter- actions across diverse fields. These studies collectively showcase the immense potential of SER in enhancing our daily lives and experiences, fostering more empathetic and interactive human–computer relation- ships. Mainly the SER tasks use speech signals to recognize the emo- tional states without requiring knowledge of linguistics. The challenges faced in this field are to extract the relevant features from the speech signals to recognize the emotional states (Batliner et al., 2011; Anag- nostopoulos et al., 2015) and to design appropriate classifiers (Ayadi et al., 2011). The importance of SER comes from the complexity of emotion recognition through audio speech for machines. Generally, the main components   of   recognizing   emotions   in   speech   are   processing   the acoustic signal given from the dataset, feature extraction, and selection and classification. This paper covered the studies on speech emotional databases, speech features, traditional ML, and DL-based approaches from 2012 to 2022. In the beginning, the traditional techniques used for linear classifiers for emotion recognition included Bayesian Networks (BN) or the Maximum Likelihood Principle (MLP), and Support Vector Machine (SVM) (Jain et al., 2020). However, the speech signal is non- stationary. Therefore, some studies consider that nonlinear classifiers work effectively for SER, for instance, Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) (Nwe et al., 2003). Many techniques have been used in the SER literature to extract and classify emotions from   signals.   DL   approaches   have   been   used   as   an   alternative   to traditional SER techniques and gained more attention in recent years. They are distinguished from conventional methods because they can capture complex features. Different approaches have been studied, like CNN, RNN, and LSTM, where they automatically realize features from audio signals. They can also extract features from spectrograms image that represents speech signals. The different general algorithms used in SER are represented in Fig. 1, where Fig. 1a illustrates the flow mechanism of ML, and Fig. 1b demonstrates the flow mechanism of DL. Most of the possible method- ologies used in SER are provided, which can be conducted in three techniques. The first is extracting handcrafted features, then using ML for classification. The second is using DL for classification, either using handcrafted features or DL extracts features automatically through layers. Finally, the sound wave can be converted into a spectrogram image and used as input for DL. While several studies and research in the SER field employed a variety of approaches and methodologies to achieve acceptable results, most of the state-of-the-art results were received by using DL techniques. Therefore, this review aimed to exam- ine different studies chosen systematically concerning criteria to focus on and cover some essential aspects of speech emotion recognition and answer research questions. The following is a list of the main contributions in this paper :  •   Identify literature related to SER from the past ten years.  •   Summarize various feature extraction methods useful in SER.  •   Summarize and compare various machine learning methods to recognize emotions from speech.  •   Identify available datasets and summarize their usefulness and limitations.  •   Discuss the SER’s possible future directions and applications to serve the human community.  Table 1  Research questions. Question   Purpose RQ1 what are the existing freely available datasets for speech emotion recognition? There are accessible databases, though there are unknown and slightly used. Therefore, summarizing what databases are available for free for the SER task will help others work on them. RQ2 what are the features used to detect emotions from speech? Understanding the orientation of some researchers in choosing the features used and determining which features were used more intensively compared to others. RQ3 what are the learning models used to detect emotion from speech? Review the models used for the training process in recognizing emotions through sound and knowing the new trend methods. RQ4 What are the metrics used to measure the performance of SER models? Checking all the evaluation metrics utilized to measure the SER model performance to discover what is being measured in emotion recognition systems and what is much used. Also, knowing the performance of each model that has been examined shows the accuracy of the recognition process in the systems.  •   Covered the SER in the natural environment and the impact of vocal burst features in SER. The rest of the paper is organized as follows: the research ob- jective and methodology discusses in Section 2. Section 3 addressed the previous surveys that discuss SER in different aspects. Section 4 provides emotional speech databases. Then, Section 5 discusses the processing step for the audio recording. Features extraction was cov- ered in Section 6. Section 7 addresses the feature selection. Section 8 discusses the models used in SER, and Section 9 mentions SER in a natural environment. Section 10 addressed the evaluation metric used to measure the performance of the SER model. Section 11 discussed the findings of the review. Limitations of the review were addressed in Section 12. Section 13 suggests future directions, and finally, the conclusion of the review is in Section 14.  2. Research objective and methodology  The primary goal of this systematic literature review (SLR) is to determine the research approaches in the SER field based on differ- ent aspects. The methodology used in SLR is based on three stages: the planning stage, the conducting stage, and finally, the reporting stage. The planning stage is divided into five stages. First, identify the research questions related to the review objectives. Second, specify the research method to extract related papers and select the proper search terms and paper selection criteria. Third, specified study selec- tion measures, including the inclusion/exclusion criteria and quality assessments. Fourth is the data extraction strategy utilized to answer the research questions raised, and the final step is synthesizing the data from the research papers that have been extracted. Fig. 2 illustrates the filtering process of selected papers for each database. It starts with 350 papers and ends with 96 papers that will be examined in this review.  2.1. Research questions  This review study aims to identify and examine articles related to SER regarding emotional speech databases, speech features, and classifiers of the traditional and new approaches. Based on that, the research questions were identified. Table 1 illustrates the research questions with the purpose.  2.2. Search strategy  The search strategy utilized for the SLR is explained in this step in terms of keywords that seek to capture all the related study papers in the domain of SER and database library resources used. A detailed illustration of the search strategy used in this survey is covered below.

Page 4:
Speech Communication 154 (2023) 102974  4 A. Hashem et al.  Table 2  The distribution according to the resource and the number of papers. Digital database libraries   ‘‘Deep neural network’’ AND ‘‘speech emotion recognition’’ OR ‘‘Audio emotion recognition’’ ‘‘Emotional speech database’’ AND ‘‘speech emotion recognition’’ OR ‘‘Emotion recognition from speech using deep learning approaches’’ ‘‘speech emotion Detection’’ OR ‘‘Speech features for emotion recognition from speech’’ IEEE   15   17   8 Science direct   16   8   11 Google scholar   7   10   4  •   Step 4: Uses quality assessment to include papers of the most elevated quality that best answer the research questions raised. The following is an illustration of the inclusion/exclusion criteria utilized in this review study:  2.3.2. Inclusion criteria  The inclusion criteria were created to review selected papers sys- tematically. Choose only papers that meet the criteria, which are: include papers that focus on SER and the emotional speech database. Include papers that use both machine learning and deep learning for the problem of SER. Only peer-reviewed articles published in the English language are considered. Include papers that have a unique contribu- tion. For duplicate publications of the same study, include only the most complete and newest. All of the published papers were included from 2012 to 2022.  2.3.3. Exclusion criteria  •   Books, notes, theses, letters, and patents will be excluded from this review.  •   Exclude papers that do not describe the methodology and result sufficiently are excluded.  •   Exclude papers that do not address the research questions men- tioned above.  •   Repeated studies are not included.  •   Exclude papers that covered emotion recognition from other than speech.  2.3.4. Quality assessment rules  The last step in choosing the list of final papers included in this review was utilizing quality assessment rules (QARs). QARs were uti- lized to assess the quality of the research papers following the study objectives. 5 QARs were identified, with each worth one point out of five. When the fully answered score was 1, the answer above average score was 0.75, the average answer score was 0.5, the below average answer score was 0.25, and the entirely not answered score was 0. The sum of all five QARs represents each paper’s overall score. A score of 2.75 or less indicates that the paper was not included in this review. The following QARs were used to assess the quality of the papers: 1. QAR1: Is the paper organized well? 2. QAR2: Are the research goals well-defined? 3. QAR3: Is the dataset used identified? 4. QAR4: Have they explained methods appropriately? 5. QAR5: Have other authors cited the study? The papers that fulfilled the inclusion/exclusion criteria and quality assessments were included. Fig. 3 represents the paper’s distribution according to year and venue.  3. Related surveys  Many surveys have been published in SER (Batliner et al., 2011; Anagnostopoulos et al., 2015; Ayadi et al., 2011; Jain et al., 2020; Nwe et al., 2003; Keele et al., 2007; Schuller et al., 2011; El Ayadi et al., 2011; Koolagudi and Rao, 2012; Swain et al., 2018; Mustafa et al., 2018; Schuller, 2018; Gunawan et al., 2018; Khalil et al., 2019;  Fig. 3.   Distribution of the 96 papers by year (a) and venue (b).  Akçay and Oğuz, 2020; Abbaschian et al., 2021; Yadav et al., 2022; Jahangir et al., 2021). Table 3 compares existing surveys to this paper regarding various aspects of SER. The current surveys discuss different aspects of SER, where the four key elements are: classifiers, features, feature selection, and databases. The earlier surveys did not cover deep learning techniques, and the focus was on machine learning. Schuller et al. (2011) included an end-to-end survey covering databases to eval- uate methods. The emotion recognition task has been addressed in their paper as both a classification and a regression problem. The databases, features, and feature selection methods have been exhaustively ex- amined. Their survey covers multiple approaches to evaluating SER models. They also emphasize research problems relating to the SER’s robustness in noisy and cross-corpus contexts, however, not addressing the methodologies. El Ayadi et al. (2011) survey covered the databases, features, and classifiers for SER, where the databases are discussed intensively. They discuss different aspects of features in terms of time local vs. global and source continuous, qualitative, spectral, and TEO-based. They also discussed combining speech features with linguistic, face, video, and discourse features to improve emotion recognition accuracy. Koolagudi and Rao (2012) conducted a survey that covered databases, features, and classifiers. They grouped the databases into three categories – acted, elicited, and natural – and covered the benefits and drawbacks of each. They divided SER features into three aspects, excitation, vocal- tract, and prosodic, and investigated the result of combining different features. For SER, various classification approaches and the combina- tion of these approaches were also covered. Both surveys did not discuss recent deep-learning approaches and issues for SER. Anagnostopoulos et al. (2015) gave a survey on SER studies from 2000 to 2011, along with an overall pipeline for SER. They empha- sized using speech (acoustic) features associated with verbal and non- verbal vocalization features, also called vocal bursts, such as laughs, cries, sighs, and yawns for emotion recognition. They discussed the importance of combining classifiers, ensemble learning, and voting techniques for emotion recognition. The feature selection algorithms were also discussed briefly. However, They did not address the current SER focus. Swain et al. (2018) give insight into SER’s databases, features, and classifier techniques from 2000 to 2017. They classified features as

Page 6:
Speech Communication 154 (2023) 102974  6 A. Hashem et al.  Most SER applications aim to be performed in real life; therefore, training the SER model with natural data type can enhance the recog- nition results. Fahad et al. (2021) thoroughly discussed all the aspects of SER tasks in the natural environment. Data types, emotion modeling, features, classifier models, mismatch environment, uncontrolled envi- ronment, and evaluation metrics were discussed in detail. Atmaja et al. (2022) survey reviewed acoustic features and discussed the bimodal SER from acoustic and linguistic information fusion. In addition, they presented and covered datasets, emotion models, features, classifiers, and fusion methods. Singh and Goel (2022) presented a systematic literature review and covered deep learning SER studies that system- atically summarized. They covered papers from 2000 to 2021 and examined 139 articles. The main objective of this survey was to answer research questions that describe mainly used databases in SER, essential features, and classifiers used. They also outlined some motivations and constraints of DL approaches for SER systems. However, Fahad et al. (2021), Atmaja et al. (2022) and Singh and Goel (2022) did not explore transfer learning techniques for SER. In my comparison, I am considering all the characteristics a good literature review paper should have, where it should describe the main parts of SER, which are database, features, and classifiers, and cover their different types of usefulness. Also, It should cover the different methods that tried to improve results and solve some SER problems. Based on that, we are comparing different literature reviews based on these points. The previous survey papers and the present survey have been compared based on the following characteristics in Table 3: (1) database, (2) emotion modeling, (3) features, (4) feature selection, (5) preprocess, (6) classifiers, (7) deep learning, (7) transfer learning, (8) SER in a natural environment, (9) vocal bursts, and (10) evaluation metrics. It shows that most previous survey papers are literature reviews and a few attempts to do a systematic literature review for SER, and all covered the main parts of SER. However, there are some limitations in covering the preprocess, transfer learning, SER in a natural environment, and evaluation metrics. Compared to other surveys, this systematic review aimed to provide a comprehensive overview of literature related to SER with reduced risk of bias and discusses the aspects of the SER, which covers all the characteristics above. The systematic literature review findings from the examined papers between 2012 and 2022 are debated based on the answers to research questions.  4. Emotional speech databases  Like all other AI tasks, data is needed to build and train a model. A good quality emotional dataset is a condition for SER (Ververidis and Kotropoulos, 2006). Many factors should be considered to evaluate the correctness of an emotional dataset. Collecting data to identify emotions is considered a challenge due to the need for human sam- ples for emotional voices. Moreover, the annotators need to label the recorded audio. Consider the size, quality of the data, and the personal characteristics of speakers, such as age and gender. Creating an emotional speech dataset is highly based on the study objective. For instance, for emotional speech synthesis, only a single-speaker emotional speech dataset would suffice. However, a dataset with nu- merous speakers and various emotional expression styles is required for emotion recognition (Koolagudi and Rao, 2012).  4.1. Databases categories  In the literature, emotional speech data is divided in terms of its creation into three types, Acted databases, evoked databases, and natural emotional speech databases (Cowie et al., 2001). These types can be divided based on emotion modality. The emotion modality represents how the annotators labeled the database collected. Each of them is generated with a different environment and characteristics as follows.  4.1.1. Acted database  One of the reliable methods of recording emotional speech datasets is acted datasets, also known as simulated datasets, where actors gen- erate them. The samples are based on acting emotions performed by professional actors like artists or people who can speak in different emotions to represent their emotional state (Ayadi et al., 2011). This type of data is recorded in control environments with high-quality au- dio and avoids recording issues like microphone distance, noise, and re- verberation. To investigate the variations in a human voice, they record the speaker speaking the exact text with various emotions in multiple sessions—morning, afternoon, evening, and night (Abbaschian et al., 2021). This base can ensure the balance of the classes. Nevertheless, in real-life applications, training data of this type may perform poorly due to the recording process. Another problem is that the samples tried to highlight the given emotion during the recording, which may be considered an exaggeration (Williams and Stevens, 1972). Popular acted datasets are EMO-DB (Burkhardt et al., 2005) and DES (Engberg et al., 1997).  4.1.2. Evoked database  Evoked emotional speech databases are data collected by creat- ing events and situations that raise the speaker’s emotions without their knowledge. The contextual states generated to produce induced emotions are artificial where, an actor converses emotionally with the speaker after producing the artificial emotional event. Tawari and Trivedi investigated (Tawari and Trivedi, 2010) the role of context in speech emotion recognition. This type is also called semi-natural because this process is not fully acted and is closer to the natural speech datasets. However, this dataset may suffer from not containing all cate- gories of emotions. These databases are occasionally recorded by asking the speakers to interact verbally with a computer, where a human con- trols it without the speakers’ knowledge (Wahlster, 2013). One of the available databases for evoked emotional speech is IEMOCAP (Busso et al., 2008).  4.1.3. Natural database  Natural emotional databases are the most challenging type to col- lect. This type is mainly recorded from real-life conversations or ex- tracted from TV shows, YouTube videos, and call center records, where sometimes it is hard to recognize emotions. This type is also known as spontaneous speech datasets. Training a model using natural emotional databases for real-life applications is the best since the model was learned from a similar environment (Fahad et al., 2021). However, this type of dataset may contain sensitive information about the speakers; therefore, it causes ethical and copyright problems, which explains why this type is not available to the public. Another problem is the lim- ited and imbalanced classes to represent emotions. Natural databases’ recordings may suffer from noise and microphone distance issues (Fa- had et al., 2021). Section 8 covers the techniques used to deal with such issues. Examples of natural emotional databases are VAM (Grimm et al., 2008) and AIBO (Steidl, 2009). Table 4 depicts an overview of each type of database and summarizes the pros and cons. Table 5 describes the emotional database in terms of language, database type, number of emotions, emotion modality, number of speakers, number of samples, and number of utterances.  4.2. Emotions modality  Choosing a model of emotion recognition relies on the availability of emotion labels in the database. The speech emotion corpus can be created from different languages and contains many emotional states. However, emotional labeling is not consistent. Based on the applications, SER systems can be modeled for classification and regres- sion problems, depending on modeling the emotion used in the given dataset. Defining and selecting emotions is a difficult task where the emotions are involved in psychological states and subjective, which

Page 7:
Speech Communication 154 (2023) 102974  7 A. Hashem et al.  Table 4  An overview of actor, evoked, and natural emotional speech databases. Type of dataset   Description   Pros   Cons Acted/Simulated   These types of databases are recorded by actors trained to represent different emotions.  ∙   Most databases are freely available.  ∙   Commonly used in SER.  ∙   It contains all types of emotions.  ∙   Easy to make and available in different languages.  ∙   This type may ensure that data and classes are balanced.  ∙   Poor performance in a real-life environment.  ∙   Exaggeration represent emotions. Evoked/Semi-natural   These databases are created without the actor’s knowledge by creating an emotional situation.  ∙   Close to the natural database.  ∙   Contain more emotions than a natural database.  ∙   The actor’s knowledge will make it like an acted database. Natural   These databases are recorded from Real-life conversations such as CallCenter or from TV shows.  ∙   No actors completely natural database.  ∙   The best for real-life modeling SER system.  ∙   Overlapping of utterances.  ∙   Mostly, this type the data and classes are imbalanced.  ∙   Noises and environment Background.  ∙   Privacy and copyright problem.  differs from one to another. Therefore, there is more than one definition of emotions, as Plutchik discussed (Plutchik, 2001). The most widely used models to represent human emotions for SER are dimensional and categorical emotions modality (Grandjean et al., 2008). It is common to label emotions into categories based on a discrete label for emotion, also known as basic emotions. According to Ekman et al. (2013); for clarity, psychologists have classified the continuous variable into six basic emotion categories: sadness, happiness, fear, anger, disgust, and surprise, where the other types of emotions are based on these categories. The emo-DB dataset is an example of categorical emotion modality (Burkhardt et al., 2005). In contrast, the idea of dimensional emotion is that every available emo- tion can be interpreted as continuous values or degrees of attributes. Also called continuous emotions, depending on the coordinates, all the existing emotions are coordinates in dimensions, mainly two- or three dimensions arousal–valence (2D) or arousal–valence-dominance (3D) (Russell and Mehrabian, 1977). Valence dimension values reflect the degree of the emotion in terms of positive or negative the highest value is the positive emotional state. Arousal dimension values reflect the activation of emotions, also called activation dimensions, where it captures the activity of the emotions with the highest value of the activity emotion state represented. Finally, the third dimension is dominance which covers the high or low of emotional power (Gunes and Pantic, 2010). Fig. 4 illustrates the representation of the circumplex model 2D valence–arousal space. As an example, high arousal and low valence can represent angry emotions. EMOV dataset is an ex- ample of dimensional emotion modality, which contains (2D) arousal– valence (Karadoğan and Larsen, 2012). The difference between the two types of human emotions modality is that the dimensional emotions depend on each other, while in the categorical, the emotions are independent of each other. Most of the emotion labels used in the datasets are categorical ones. However, this type cannot capture the complex emotion as the dimensional does (Mehrabian and Russell, 1974). However, the weaknesses of the dimensional model are that some emotions, such as fear and anger, become equal in dimensional representation. Surprise emotions, for example, cannot be classified and exist beyond dimensional space because they can have either a positive or negative valence based on the context (Zeng et al., 2007). Table 5 represents the emotions modality of the available emotional speech databases. Not all databases in the studies are public and freely available to everyone. Some have restrictions and are not available. Others need a license to be used or accessible with a price. For example, the data used from call centers or those recorded and taken from real life are probably private datasets because they may contain the personal information of the speakers. A summary of databases based on the data modality is presented in Table 5. The selection of databases depends on the most important and commonly utilized in SER.  Fig. 4.   The distribution of the basic emotions on the Valence–Arousal dimensions emotional model.  5. Speech processing  After defining the database, to build an emotion recognition system from the audio, there is a need to preprocess the available sample. Preprocessing is a significant step that refers to procedures that must be performed on speech signal samples (Nema and Abdul-Kareem, 2018). SER systems are more complex processes in which recognition accuracy depends on several stages. As mentioned in the earlier section, the data utilized in the entry affects the results extracted. Therefore, in the first step, and before extracting features from the audio clips, the sample signals must be preprocessed to improve the existing data as much as possible to focus on salient features and to facilitate subsequent operations such as removing silent intervals and pre-emphasis, where some studies utilized the pre-emphasis step, which is used to boost the signal’s high frequencies by using a pre-emphasis filter (Zayene et al., 2020). These processes can be done for feature extraction. Others can be for normalizing the features where all utterances must undergo some energy normalization due to variations in recording environments, or some specific characteristics of the speakers are removed, such as the gender and age of the subjects (Nema and Abdul-Kareem, 2018). Fig. 5 illustrates the five processing steps discussed.  5.1. Signal framing  Framing is also known as speech segmentation. Framing divides speech samples between 20 and 40 ms, collected from analog to digital

Page 10:
Speech Communication 154 (2023) 102974  10 A. Hashem et al.  Fig. 6.   The segmentation of the signal with an overlapping.  manifest at various temporal scales, and capturing these cues within appropriate time windows is crucial for accurate emotion recogni- tion (Jalal et al., 2020). The time window for which emotion recog- nition is relevant in SER can vary depending on the type of feature being used. Research has shown that emotions can be recognized within relatively short time windows, typically ranging from a few hundred milliseconds to a few seconds. For example, studies have shown that vocal expressions of emotions can be recognized within 1–3 s (Scherer, 1995), while acoustic features such as pitch and intensity can be ana- lyzed over a shorter time (Eyben et al., 2010). Short windows of audio data are often used to form a feature vector for classifying the whole utterance. These feature vectors can be based on various acoustic fea- tures, such as pitch, loudness, spectral energy distribution, and cepstral coefficients (Boateng et al., 2022). There are many techniques to com- bine information from multiple time frames that can be used for SER to determine the utterance-level emotion for each input utterance, includ- ing temporal feature integration, which involves combining features extracted from multiple time windows to capture temporal dynamics in the data (Ntalampiras and Fakotakis, 2011), majority voting which involves aggregating the predictions of multiple classifiers to make a final prediction (Kurpukdee et al., 2017), and deep learning approaches which involve training neural networks to automatically learn features from raw data (Raj et al., 2023; Jiang et al., 2019). The tradeoff between time and frequency resolution in SER is influenced by the size of the time window used for analysis, with more oversized windows providing better time resolution but sacrificing frequency resolution and vice versa for smaller windows (Yang et al., 2019). Several studies have explored the optimal time window for different acoustic features. Paliwal et al. (2010) study concludes that the optimal window duration for speech analysis is between 15–35 ms, which is especially suitable for speech applications relying on the short-time magnitude spectrum (Puterka and Kacur, 2018). The study investigated the impact of speech signal duration on emotion recognition, testing segments of varying lengths. Longer speech segments demonstrated higher accuracy, with a significant difference of over 17% between the shortest and longest sequences. Puterka et al. (2019) aimed to determine whether it is better to use shorter or longer windows that identify the optimal parameters for SER classification, explicitly investigating the appropriate time and frequency resolution of spectrograms. Based on the obtained results, it can be inferred that utilizing a window length of 10 ms with a 50% overlap of neighboring frames is sufficient to achieve accurate classification into seven emotional classes. It concludes that the optimal time window for SER depends on the context and the type of emotion recognized. Shorter time windows may be more helpful in capturing dynamic changes in emotion, while longer time windows may be more beneficial for capturing overall trends in emotion. In other words, the time window for which emotion recognition is relevant can vary depending on the specific application and context. In some cases, emotion recognition may be relevant in real-time, where emotions must be detected and recognized as they occur, such as in customer service interactions, virtual reality experiences, or driver safety systems (Re- quardt et al., 2020). In other scenarios, emotion recognition may be relevant for analyzing emotions over extended periods, such as in healthcare settings for monitoring and diagnosing patients’ emotional states or in market research to understand consumer sentiment and preferences over time (Han et al., 2020; Madanian et al., 2022). The relevant time window for emotion recognition will depend on the specific goals and requirements of the application. Some applications may require immediate and real-time recognition, while others may focus on analyzing emotions over extended periods to gain insights and make informed decisions. Table 6 compares different time window sizes used in SER studies along with the best performance results achieved. Most of the papers explain the window size as a pre-process step with no justification; however, when the time window increases between 60–265 MS, its possible that the study wants to focus more on frequency information in each frame since the frequency resolution is higher, and when the time window is less between 10–40 MS it is possibly that the study wants to focus more about temporal information in each frame since the temporal resolution is higher. Most studies used a time window size of 25 ms, with different features, models, and datasets such as FAU AIBO, IEMOCAP, EMO-DB, EMOVO, and SAVEE. The reason for using this size goes to extract short-term information. Also, some studies assume that 25 ms contains sufficient emotional information (Han et al., 2014). The time window size of 20 MS was chosen in Ancilin and Milton (2021) because a longer frame duration can lead to changes in signal characteristics within the frame. The method was evaluated using a 10-fold cross-validation, resulting in an accuracy of 81.50%. After the experiment, Gilke et al. (2012) found that the highest mean classifica- tion score gave with 23.22 MS. It is still an open problem to determine the appropriate analysis window for emotion recognition. Fortunately, a speech segment over 250 MS has been shown to contain sufficient emotional information (Kim and Provost, 2013). These varying results indicate that the choice of window time size is crucial but not the only factor affecting accuracy but also the data type, model, and metrics used. From another perspective, considering the extent of acoustic infor- mation necessary for a listener to recognize vocal emotional expressions has been a subject of inquiry in the research. In an early investigation, Pollack et al. (1960) conducted a study on vocal emotion recognition, where they observed that recognition accuracy improved with longer gate lengths. They found that recognition of 8 distinct ‘‘modes of expression’’ exceeded chance levels for utterances as short as 60 ms. It is worth noting that while fear and happiness were among the included emotions, most of the ‘‘modes’’ were non-emotional, such as statements, questions, and confidential communication. However, their study needed more specific data reporting for individual emotions, and the limited methodological details provided hindered a comprehen- sive evaluation of their findings. Audibert et al. (2007) explored the perception of various affective states using six gates to divide single- word utterances. In the previous vocal gating studies reviewed (Cornew et al., 2010; Lacheret et al., 2007; Jiang et al., 2015; Pell and Kotz, 2011; Rigoulot et al., 2013) negative emotions, such as anger, fear, and sadness, have consistently demonstrated faster recognition com- pared to happiness. Notably, most of these studies have predominantly focused on happiness as the only positive emotion investigated, except for Audibert et al. (2007). More recently, Nordström and Laukka (2019) study significantly broadens the scope of previous gating investigations by incorporating an extensive repertoire of vocal expressions such as anger, fear, happiness, interest, sadness...etc. They explore the rapid recognition of emotions from speech and music by humans. The study aimed to explore the time course of emotion recognition in speech and music. Emotions are an essential aspect of human communication, and understanding how they are perceived and recognized is crucial for various fields, including psychology, linguistics, and communication sciences. The researchers employed a gating paradigm, a method com- monly used in studies of emotion recognition, to estimate the amount of acoustic information required for accurate emotion recognition. This paradigm involves presenting stimuli in sequential segments or ‘‘gates’’,

Page 11:
Speech Communication 154 (2023) 102974  11 A. Hashem et al.  Table 6  Comparison between different time window sizes used in SER literature. (MS: Millisecond, WA: Weighted accuracy, UA: Unweighted accuracy, UAR: Unweighted Average Recall). Time window size   References   Best performance 20 MS   Ancilin and Milton (2021)   Accuracy: 81.50% 23.22 MS   Gilke et al. (2012)   WA: 79.55% 25 MS   Zayene et al. (2020), Zhao et al. (2021b), Yi and Mak (2020), Guo et al. (2018), Alam et al. (2013), Han et al. (2014), Bertero and Fung (2017), Lee et al. (2019), Hsu et al. (2021), Li et al. (2021a), Zhang et al. (2021) and Liu et al. (2022). WA:73.1% UA:66.3% (Zhao et al., 2021b) WA:84.49% UAR:83.31% (Yi and Mak, 2020) Precision:93.30%, Recall:91.97%, F1:92.50% (Guo et al., 2018) 30 MS   Li et al. (2021c)   UW: 56% WA: 61% 40 MS   Dai et al. (2019) and Zou et al. (2022)   UW:60.98% WA:58.93%, WA:71.64% UA:72.70% 64 MS   Aftab et al. (2022)   UW: 94.15%, WA: 94.21%, F1: 94.16% 65 MS   Fan et al. (2021)   WA: 62.4%, UA: 42.9%  starting from very brief excerpts and gradually increasing the duration of the stimuli. By doing so, they could pinpoint the shortest gate duration at which participants could accurately recognize each specific emotion. Additionally, the study analyzed the Emotional Identification Points (EIPs), representing the time when recognition rates stabilize for each emotion. The study included a diverse range of emotions in speech and music to ensure a comprehensive investigation of emo- tion recognition. The emotional expressions encompassed a variety of valence (positive and negative) and arousal levels, allowing for an exploration of how these factors influenced recognition speed. The findings revealed exciting patterns in emotion recognition. While some emotions were quickly recognized from brief stimuli, others required longer durations for accurate identification. Negative emotions, such as anger, fear, and sadness, appeared to be recognized faster than positive emotions, except for happiness. This aligns with previous research indicating that negative stimuli tend to draw more attention due to their potential threat value. Moreover, the study shed light on the acoustic features that enable rapid emotion recognition from speech and music. It suggested that low-level physical characteristics, such as F0 level, voice intensity, and voice quality, played a crucial role in dis- criminating between emotions. These features were rapidly expressed and detected within a few glottal cycles, emphasizing the efficiency and speed of emotional communication through speech prosody and music. However, the results also highlighted an intriguing exception: the emotion of relief. Relief recognition required stimulus durations significantly longer than other emotions, implying that it might rely on cues that develop over time or emerge toward the end of an utterance. This finding opens up new avenues for investigating the temporal dynamics of specific acoustic cues associated with relief and exploring how they contribute to emotion recognition. The study by Nordström and Laukka (2019) provided valuable insights into the time course of emotion recognition in speech and music. Using the gating paradigm allowed for a detailed analysis of how quickly various emotions can be identified from acoustic information. The findings suggest that acoustic cues play a vital role in emotion communication and that different emotions may exhibit distinct recognition patterns. Further research in this domain can contribute to a deeper understanding of human emotional processing and its implications in various fields, such as emotion recognition technology, human–computer interaction, and affective computing.  5.3. Voice activity detection  Generally, an utterance consists of three elements: voiced, unvoiced, and silent. The voiced utterance is produced by the vocal folds vi- brating, which periodically excites the vocal tract during phoneme pronunciation. The phonemes are perceptually different units of sound that differentiate between words. Unvoiced speech results from air flowing through a constriction in the vocal tract, creating noises that are brief and turbulent and are caused by periodic excitations of the vo- cal tract. Recognizing and extracting spoken communication is possible because of its periodic character. Endpoint detection, speech detection, or vocal activity detection are terms used to identify voiced speech among other unvoiced speech and silence (Lokhande et al., 2012). The voice Activity Detection algorithm’s performance impacts the system’s accuracy. Suppose speech has many noisy and silent frames, which raises computing complexity. Therefore, removing silence frames with less energy depending on a certain threshold makes modeling speech easier and reduces the complexity (Yiming and Rui, 2015). The auto- correlation method, short-time energy, and zero crossing rate are the most often utilized techniques for vocal activity detection (Lokhande et al., 2012). Nevertheless, some studies attempt to use unvoiced speech and silence features to enhance the result of SER, which is covered in Section 8.  5.4. Normalization  Normalization can be done on the features level or corpus level. The idea of feature normalization is to modify the volume sound into a standard level by computing the maximum value of the signal and then dividing the entire signal sequence by this maximum (Nema and Abdul-Kareem, 2018). The benefit of this step is that it enhances the generalization ability without losing the discriminative strength of the features by reducing the recording and speaker variability (Sun and Wen, 2015). The best-known normalization is Z-normalization (Auck- enthaler et al., 2000), which mathematically can be represented in Eq. (2), where   𝜇   is the mean, and   𝛼   is the stander deviation of the speech signal.  𝑍   =   𝑥   −   𝜇 𝛼   (2)  5.5. Noise reduction  The amount of noise in the signals depends on the data type, where the noise in the speech signals is inevitable. However, it may exist more in the natural speech data since it is taken from real life in different environments. This noise interferes with the sound waves, negatively impacting the recognition process and resulting in poor performance accuracy, which happens because the audio signals may be corrupted by the different types of noise (Thu et al., 2019). Various techniques have been used to reduce this noise, also called unwanted signals. Denoising techniques are applied to reduce unwanted signals from audio. Such techniques used are minimum mean square error (MMSE) and log-spectral amplitude MMSE (LogMMSE), which are considered the most successfully used techniques for denoising, where it improves the results (Pohjalainen et al., 2016).

Page 12:
Speech Communication 154 (2023) 102974  12 A. Hashem et al.  Fig. 7.   Classification of acoustic features for Speech Emotion Recognition, where it can be extracted by using traditional Handcrafted Features using low-level descriptors(LLD) or high-level statistical functions (HSF), or both. The second way is using machine/deep learning.  6. Features extraction  After the speech signal processing, the next step is feature ex- traction, which is also one of the crucial parts affecting recognition performance. This step aims to find features that highlight and ade- quately discriminate different emotions. The features extracted from the audio speech signal are called acoustic features. There has been extensive research on the relationship between acoustic characteristics and emotion (Scherer, 2005; Mairano et al., 2019). The extracted features do not deviate from one of these types: frequency-domain features, time-domain features, statistical features, hybrid features, and deep features. The frame involved in the process determines the feature extracted. The speech signal is segmented into frames, and features are extracted from these frames. Feature extracted from the direct process of the time domain waveform is called a time do- main feature, like short-time zero crossing rate, short-time energy, and pitch frequency (Lee and Narayanan, 2005). The Fourier transform, wavelet transform, and other methods are employed for the time- domain data to create the frequency-domain, from which the feature is retrieved, and those features are connected to how people perceive speech. Spectral features on 6.2 go into further depth. The statistics extracted are based on an utterance level, while the earlier ones are based on the frame level (Rao et al., 2013). Utterance-level features more deeply represent speech’s emotional characteristics (Gao et al., 2017; Ntalampiras and Fakotakis, 2011). Standard statistical features are based on statistical processes. Deep features are obtained by feeding the deep neural network with the original voice signal or spectrum. Hybrid features combine all types of features mentioned to create a feature set. For example, the GeMAPS (Eyben et al., 2015) feature set contains 62 generated from 18 time and frequency-domain char- acteristics. The eGeMAPS is a GeMAPS extension with 88 features, five spectrum features, and 18 from both time and frequency-domain features. Acoustic features taken from each frame were used in tra- ditional handcrafted features (Pepino et al., 2021). Local features or low-level descriptors are common names for these features (LLDs). The dynamics between frames can be captured by calculating the statistical features computed from LLDs, also named global features, suprasegmental features, high-level features, or high-level statistical functions (HSFs) (Gao et al., 2017; Ntalampiras and Fakotakis, 2011). Fig. 7 depicts the classification of acoustic features for SER. The handcrafted acoustic features are two types: Low-Level Descrip- tors (LLD) and global features. First, the local features are extracted from frames. Then, as described in speech preprocessing Section 5, the LLD features are extracted from the quasi-stationary segments, representing the approximated value they are essential in which the emotional features are not evenly distributed across the entire speech signal. Therefore, it is also known as segmental or short-term fea- tures, covering the temporal dynamics. Global features are statistical descriptors like mean, min and max values, and standard deviation. It is derived from the local features and computes the statistical importance of the local features among all frames to extract the utterance features; therefore, its also called long-term or supra-segmental features (Nta- lampiras and Fakotakis, 2011). Also, extracting global or local features or both is possible depending on the need. The four speech features categories discussed are illustrated in Fig. 8  6.1.   Prosodic features  They   are   also   called   para-linguistic   features   or   suprasegmental features because the speech analysis’s frame period is generally 30– 100 ms, covering human perception, for instance, rhythm, fundamental frequency, duration, and intonation. These features are related to the speech unit, such as in sentences, words, syllables, and expressions; therefore, they are long-term features (Swain et al., 2018). They corre- late well with emotions like pitch, duration, and energy (Schroder and Cowie, 2006). Koolagudi and Rao (2012) organized prosodic features into three classes pitch, intensity, and intonation, and these types of features are based on the air pressure of the vocal fold. It is helpful to use the prosodic statistical value features, such as pitch, mean, minimum, and maximum, to identify the emotion where they hold important emotional information (Daneshfar et al., 2020; Li and Akagi, 2019). These features can differentiate between emotions with different arousals, such as happy and sad. In contrast, they cannot distinguish between emotions with the same arousal, like anger and happiness, the emotion related to the valence (Fahad et al., 2021).  6.2. Spectral features  The speech signals produced by the speaker mean that the vocal tract is controlled. Therefore, the vocal tract features can be employed to obtain spectral features (Fleischer et al., 2015). Spectral features are extracted from the frequency domain signal; therefore, the time domain signal needs to be transformed using Fourier transform (FT) (Brigham and Morrow, 1967). Due to the speech analysis’s frame duration, which is about 10–30 ms, spectral features are sometimes referred to as segmental features. The Fourier transform aimed to convert amplitude as a function of time to amplitude as a function of frequency by a mathematical operation (Heinzel et al., 2002). The common feature used in this category is Mel Frequency Cepstral Coefficients (MFCC),

Page 14:
Speech Communication 154 (2023) 102974  14 A. Hashem et al.  Table 7  Comparison between deep learning automatic and traditional handcrafted features approaches. Characteristics   Deep learning automatic features   Handcrafted features Extracted and represented features   Automatically captured the features from the row wave signal and learned the ability to learn the best pattern to enhance classification accuracy. Based on manually extracted features that depend on domain expertise and cannot capture complex features. Variety and Generalization   Aids in automatically capturing spatial, temporal, and local dependencies from unlabeled speech data. Require to use of labeled emotional speech data, feature selection, and dimensionality reduction techniques that are difficult to generalize. Data preprocessing   Deep learning features do not require data preprocessing or normalization to achieve better results. To improve classification accuracy, the features need extensive preprocessing, normalization, and post-processing. Changes in speech signals (spatial & temporal)   The issue of intra-class variability and inter-class similarity present in handcrafted features are overcome by presenting hierarchical and translational invariant features. Handcrafted features are not capable of handling inter-class variabilities and inter-class resemblances. Time for model training and execution   It is needed for a huge emotional speech database to avoid overfitting, while it needs a high computational resource to enhance the training speed. Smaller training data sets with less computation time and memory usage is required.  speech features. However, due to the different airflow and resulting vortices spread throughout the vocal tract, according to Teager and Teager (1983), this idea may not be valid. Therefore, nonlinear speech features are crucial for classifying different emotional states. The initial implementation of the Teager Energy Operator (TEO) was based on evidence supporting the notion that hearing is an energy detection method, as proposed by Teager (1980) and Kaiser (1990). Fundamental frequency changes have been observed in stressful situations, such as the arrangement of harmonics among critical bands (Teager and Teager, 1983). Furthermore, TEO applied to a speech signal containing multiple frequencies can represent the interactions between specific frequency components. As a result, speech features based on the TEO can be utilized for stress classification. It is generated from the speech signal using the nonlinear equation in Eq. (3), where   𝜑   is the TEO and  𝑓   ( 𝑛 )   is the speech signal.  𝜑   [ 𝑓   ( 𝑛 )] =   𝑓   2 ( 𝑛 ) −   𝑓   ( 𝑛   − 1)   𝑓   ( 𝑛   + 1)   (3) TEO-based features, i.e., TEO critical band auto envelope normal- ized, TEO auto envelope, and decomposed TEO frequency modulation variation, are the three TEO-based features utilized to differentiate between neutral and stressed speech and classify stressed speech into three categories: angry, loud, and Lombard (Zhou et al., 2001). Deep learning algorithms have gained popularity in recent years due to their ability to automatically learn intricate patterns and rep- resentations from large amounts of data. Unlike traditional machine learning methods, which can capture complex hierarchical relation- ships in the data, making it particularly effective for tasks involving high-dimensional and unstructured data, such as speech processing. By using DL techniques to extract acoustic features of speech in an end- to-end manner, by taking advantage of these algorithms’ capacity to discern subtle variations and nuances in the audio signal, leading to more accurate and robust emotion recognition models. Moreover, DL models have demonstrated exceptional performance in various audio- related tasks, including speech recognition and music analysis, rein- forcing the rationale for leveraging these techniques in the context of emotion recognition from speech prosody; therefore, it is reasonable to use deep learning techniques to extract acoustic features of speech in an end-to-end method (Tang et al., 2018). The LLD features are directly fed into the deep learning models, such as a convolutional neural network (CNN), a long-short-term memory network (LSTM), or a combination of both, where the algorithms learn more complex understandings of essential features. For instance, a CNN network learns spatial features, whereas an LSTM network learns the tempo- ral relationship between features. DeepSpectrum and AuDeep were two deep learning-based features included in the baseline system of the INTERSPEECH 2020 ComParE challenge (Schuller et al., 2020). DeepSpectrum is a Python toolkit for extracting features from audio files using pre-trained images (CNNs), whereas AuDeep is a Python toolkit for acoustic data deep unsupervised representation learning. It is based on a recurrent sequence-to-sequence autoencoder technique that can learn time series data representations while accounting for their temporal dynamics (Freitag et al., 2017). Feature extraction is a crucial phase in SER and is vital in reducing computational complexity and classification errors. The feature extrac- tion process can be performed manually or automatically, as men- tioned. However, manually extracted features are domain-dependent, cannot construct efficient models, and heavily rely on domain expert knowledge and the studies in SER have indicated that there is no universally ideal discriminative feature that accurately represents emo- tions across different databases (Jahangir et al., 2021). In contrast, DL techniques with multiple layers offer the advantage of automat- ically extracting features from raw speech data and learning ideal patterns to enhance classification performance. DL-based automatic fea- ture extraction has demonstrated remarkable performance gains over traditional handcrafted features for SER (Papakostas et al., 2017; Zheng et al., 2015). These DL-based techniques enable learning translational invariant features and local dependencies without relying on domain expert knowledge. A comparative analysis between DL automatic and handcrafted features extraction approaches for SER, summarized in Table 7, considers different parameters (Nweke et al., 2018). The find- ings show that DL-based automatic features possess distinct advantages over traditional handcrafted features, mainly when dealing with com- plex classification tasks. The ability of DL techniques to autonomously learn informative representations from speech data makes them highly beneficial for SER applications.  7. Feature selection  After features extraction and due to the massive number of available features where up to date, there are no features set that ensure the reflection of the emotion. Therefore, a selection algorithm is essential to optimize the extracted feature, reduce features, and highlight relevant and useful features from a set of features without changing the original feature values. This process is called feature selection and dimension reduction. This phase eliminates features that are not correlated with emotions and discard repetition ones. The advantage of feature selec- tion techniques is to enhance accuracy and decrease computational complexities, as the well-known problem of the curse of dimension- ality is avoided (Lee et al., 2002). Filter methods like correlation algorithms (Elbarougy, 2019), wrapper methods like forward selection and backward selection (Maldonado and Weber, 2009), and embedded methods (Lal et al., 2006) are the three types of feature selection tech- niques. The difference between these three types is that filter methods do not include learning. In contrast, wrapper methods employ ML to evaluate the quality of feature subsets without requiring any knowledge of the classification or regression function’s particular structure. Unlike

Page 15:
Speech Communication 154 (2023) 102974  15 A. Hashem et al.  Fig. 10.   Classification model categories used in speech emotion recognition.  filter and wrapper approaches, embedded methods do not separate feature selection from the learning process (Lal et al., 2006). In earlier studies, the methods used for feature selection are sequential backward selection (SBS), sequential forward selection (SFS) (Özseven, 2019; Jing et al., 2018; Wang et al., 2020), Linear Discriminate Analysis (LDA), and Principal Component Analysis (PCA) (Özseven, 2019; Jing et al., 2018), fisher feature selection method (Sun et al., 2019a). Optimization techniques, such as particle swarm optimization (Chen et al., 2020), have also been used in feature selection. Sheikhan et al. (2013) used a combination of ANOVA and Tukey selection methods. Wrapping features can be done using open-source software platforms such as WEKA or RapidMiner (Jovic et al., 2014). Recently, to decrease the engineering effort, an end-to-end SER was proposed, which solves the issues of feature selection and reduces hyperparameter tuning (Pandey et al., 2019).  8. Speech emotion recognition classifiers  Emotion recognition has been studied for decades, and the clas- sifiers used for SER have the same importance as speech features. Many methods and algorithms have been used to solve and cover different aspects of the problem and can be organized into traditional ML classifiers and deep learning classifiers. Fig. 10 illustrates the classi- fication model categories used in SER systems. Some studies utilized a hybrid type based on combining traditional methods with deep method classifiers (Singkul and Woraratpanya, 2022). In addition, many studies have tested different methods for SER systems. Nevertheless, to date, no specific model for emotion classification leads to superior performance for all conditions, and frequently, appropriate classifiers are chosen based on prior research. This section covers the publication on the classifiers in terms of traditional ML and DL approaches.  8.1. Traditional machine learning approaches  In a past effort to recognize emotion from audio signals, they had to pass through several phases covered in the earlier sections. After extracting features from speech signals, earlier studies were based on classic ML approaches to classify emotions. There were a variety of ML algorithms used for speech emotion recognition by researchers. For instance, support vector machines (SVM) (Yildirim et al., 2021; Ancilin and Milton, 2021; Sinith et al., 2015). SVM aimed to find the optimum classification results by mapping the input into a higher dimensional space based on the kernel function (Seehapoch and Wongthanavasu, 2013). Jain et al. (Jain et al., 2020) extracted Pitch, MFCC, LPCC, Energy, and speech rate as a feature and used SVM to categorize four emotional states from two datasets, and they proved that MFCC gave better results than LPCC, with an accuracy of 85%. Other classifiers, such as the Gaussian Mixture Model (GMM), define each emotional class’s probability distribution using a mixture of Gaussian compo- nents (Alam et al., 2013; Bie et al., 2013), and Hidden Markov Model (HMM), which has the benefit of being capable of presenting the tempo- ral dynamics of emotions (Chaudhari and Alex, 2016). Li et al. (2015) aimed to explore how different features like spectral, voice quality, and prosody influence emotion recognition. They adopted widely used classifiers such as SVM, CART (J48), and random forest, which gave the best result. Liu et al. (2018) used the extreme learning machine (ELM), SVM, BPNN, and KNN classifiers on CASIA datasets, and a novel feature selection approach based on Fisher and correlation analysis was presented. Achieved accuracy levels of 89.9%, 87.20%, 82.30%, and 80.70%, respectively. The authors in Özseven (2019), Yildirim et al. (2021), Manohar and Logashanmugam (2022) and Mencattini et al. (2014) proposed different feature selection techniques and used ML to classify emotional states among different datasets. Wavelet Packet Features had been covered in Wang et al. (2020) and Shah et al. (2017) and used SVM and ELM, respectively. A hybrid technique was used in Henríquez et al. (2014) and eval- uated on three databases. In addition, despite the computational com- plexity, some studies tried to enhance recognition performance by com- bining several classifiers called ensemble methods (Chalapathi et al., 2022). The SER ML approach commonly relies on SVM, GMM, or HMM classifiers, which leverage their advantages. SVM has a strong general- ization capability and robustness against outliers and high-dimensional features and demonstrates reliable performance even without feature selection algorithms (Seehapoch and Wongthanavasu, 2013; Jain et al., 2020; Yang et al., 2017). The statistical classifiers like GMM and HMM (Schuller, 2011; Přibil and Přibilová, 2013; Schuller et al., 2003) are widely utilized in SER. GMM is a straightforward classifier that captures the probability distribution of each class using a mixture of Gaussian components, essentially resembling a continuous HMM with only one state (Schuller, 2011; Alam et al., 2013; Bie et al., 2013; Zão et al., 2014). On the other hand, HMM is particularly effective in modeling the temporal dynamics of emotions (Chaudhari and Alex, 2016; Schuller et al., 2003; Lin and Wei, 2005). Table 8 shows their advantages and disadvantages. The literature on traditional techniques are examined in terms of databases used, the recognized emotions num- ber, the feature set used, the classifiers used, and the average accuracy achieved. An analysis is conducted on the existing research regarding conventional methods, which encompasses the databases utilized, the number of emotions recognized, the feature sets employed, the types of classifiers applied, and the accuracy achieved. Table 9 summarizes the literature on traditional SER techniques. In light of the above findings and the overview of the literature presented in Table 8, it can be concluded that most papers have used SVM as a classifier, and the best accuracy of 97.57% is achieved using time–frequency features on the EMODB database. In addition, most papers use spectral features or a combination of spectral features with other feature types as input for emotion recognition. The EMODB dataset was the most frequently employed evaluation dataset for SER models, followed by IEMOCAP.  8.2. Deep learning approaches  Currently, most models used in SER include DL, which has been established to provide better results than ML (Sefara and Mokgonyane, 2020). DL approaches can learn hierarchical high-level representation from raw features, flexible functions, a complex and vast range of feature learns, scalability, and recognition rate (Lee et al., 2019). The model can automatically learn appropriate features and do the classification task; however, it requires a large dataset in the learning process (Zhou et al., 2016). Therefore, different DL approaches were utilized in SER systems, where each has advantages and restrictions in some aspects. A neural network with only one hidden layer is named a shallow network, such as an extreme learning machine (ELM) (Huang et al., 2006). In contrast, if it has multi-hidden layers, it is called a deep network, such as CNN and recurrent neural networks (RNN). In SER, CNN is suitable for capturing spatial features, while RNN is for

Page 17:
Speech Communication 154 (2023) 102974  17 A. Hashem et al.  Table 10  Deep learning approaches utilized for speech emotion recognition. Deep learning approaches   Short description Convolutional neural network (CNN) CNN architecture consists of input in the first layer, followed by the convolution functions that extract LLD features, then an activation function and a pooling layer. The pooling layer aimed to reduce the domination of the features by selection mechanism based on the pooling type, e.g., max pooling. The next step contains the same order as the previous one and is fed to the multi-layer perceptron. Finally, a dense layer is followed by the activation function, e.g., SoftMax, to classify the output. CNN is mainly used for tasks that include image processing (Bengio et al., 2007). Recurrent neural network (RNN)   RNNs are neural networks where the input for the current step is derived from the output of the previous step, which makes it suitable for sequential data where it learns the temporal events, such as NLP and speech processing, because of the feedback connection to the previous layer (Rumelhart et al., 1986). However, RNN trained backpropagation through time; therefore, it may face an explosion or vanishing problem (Weninger et al., 2016). Long short-term memory (LSTM)   To overcome the limitations of RNN, LSTM was introduced. LSTM units are based on a cell, and three gates, input, output, and forget gates. The cell aims to remember values over time, while the gates control the flow of information (Hochreiter and Schmidhuber, 1997). Bidirectional long short-term memory (BiLSTM) Bidirectional adds one more LSTM layer, which changes the information flow’s direction where the input sequence flows back in the additional LSTM layer and combines the outputs from the two LSTM layers in various methods, including average, sum, multiplication, and concatenation (Siami-Namini et al., 2019). Auto-encoders   It is a generative deep-learning technique. It covers the unsupervised learning idea. Besides the input layer, auto-encoders are based on the two-element encoder and decoder layers. Among several auto-encoders, variational autoencoders (VAE) and Denoising autoencoders (DAE) are utilized in SER (Eskimez et al., 2018). Attention Mechanism (AM)   AM have been utilized widely in SER applications, where the idea behind it is that it is tried to pay attention to the crucial feature by giving it more weight than others. Therefore, it is beneficial in SER, where it can extract the salient features (Vaswani et al., 2017).  tasks is the wav2vec feature set. Xia et al. (2020) aimed to learn significant segments for recognizing emotions in speech. The approach involves extracting LLD features and dividing them into equally over- lapping segments. Each segment will be passed to a DNN to estimate the emotion probabilities. Then, with self-attention, an attentive temporal pooling module composed of a GMM and an auxiliary DNN is used to learn the emotional saliency weights of each frame. Speech inputs were transformed into spectrograms in Mansouri et al. (2022), Ntalampiras (2021), Stolar et al. (2018, 2017) and Fayek et al. (2015), and CNN was used to identify the emotions. Kwon et al. (2021) extract spatial information from a spectrogram using MLP and CNN for SER; the proposed model shows progress, where it decreased the overall complexity and enhanced the recognition rate on IEMO- CAP, RAVDESS, and EMODB datasets. Dai et al. (2019) proposed an approach to learn discriminative features from spectrograms of varying lengths for emotion recognition by combining softmax cross-entropy loss and center loss and also presented the advantages of utilizing spectrograms as input features. The speech emotions were identified by authors in Atila and Şengür (2021), Zhao et al. (2021b), Zhang et al. (2021), Zhao et al. (2021a) and Zayene et al. (2020) using spectrograms as an input in 2D,3D CNN-based SER models. CNN with other classifiers have been used in Lim et al. (2016) and Guo et al. (2018) and enhances the performance in terms of accuracy. The authors in Zhao et al. (2019b), Atila and Şengür (2021), Li et al. (2021c) and Lim et al. (2016) created SER methods by combining CNN and LSTM classifiers and tested them on various databases. Zhao et al. (2019a) an attention-based bidirectional LSTM was combined with an attention- based fully convolutional network to capture spatiotemporal features from spectrograms. A hybrid approach was used in Manohar and Logashanmugam (2022), Daneshfar et al. (2020), Sun et al. (2019b), Wang and Tashev (2017), Guo et al. (2018) and Han et al. (2014) and evaluated on different databases. Li et al. (2013) proposed a hybrid DNN+HMM; they extracted MFCC features and trained them on two different datasets. The authors in Mirsamadi et al. (2017) and Xia et al. (2019) focused on extracting the sequential features and used an RNN classifier. To overcome the limitation in RNN, researchers used LSTM and BiLSTM in Atmaja and Akagi (2020), Joshi et al. (2020), Huang et al. (2019) and Hsu et al. (2021)since they are better than RNN in learning and remembering over long sequences of inputs. Furthermore, Luo et al. (2017) utilizes an ensemble of LSTM blocks and a wide residual network for classification. Nevertheless, as a result of imbalance and difficulties in designing the classifier, it demonstrated only minor im- provements over CNN-based models. Jiang et al. (2019) suggested an SER system based on RNN. The system used BiLSTM to extract high- level emotional state representations from the temporal dynamics of emotions on EMO-DB and CASIA datasets, which improves recognition accuracy. Different attention mechanisms (AM) types have been used in SER (Wang et al., 2022; Chen and Huang, 2021; Liu et al., 2022; Zou et al., 2022; Zhao et al., 2021b; Li et al., 2021a; Zhao et al., 2021a). Attention is a technique used in DL, but it can also be utilized independently of Guo et al. (2022) combined AM into a multi-channel model (MCMA) to focus on the information relevant to SER and ex- tract more salient emotional representations. Mirsamadi et al. (2017) presented a novel weighted-polling method motivated by the AM of neural machine translation. Chen and Huang (2021) utilized a dual attention-based Bi-LSTM mechanism with hybrid features on the IEMO- CAP dataset. The created model’s architecture can more effectively learn to differentiate emotional information’s features. Feature fusion techniques have been adopted in Zou et al. (2022), Wang et al. (2022) and Pepino et al. (2021) for additional development of the SER systems. Xu et al. (2022) used a CNN to obtain visual data from the spectrogram and extract statistical features using a DNN model. Moreover, temporal features derived from MFCC were extracted using LSTM, and fusion of these features was employed on EMO- DB and IEMOCAP for SER. An integrated framework of DNN, CNN, and RNN is molded in Yao et al. (2020) with LLD, HSF, and Mel- spectrogram features extracted from the IEMOCAP dataset. Wang and Han (2019) a method was suggested that utilizes a combination of deep and shallow neural networks for speech emotion recognition. The proposed approach involves using a deep belief network (DBN) to au- tomatically extract and identify speech emotion components, followed by a shallow neural network to provide the final recognition results. Deng et al. (2017b) Proposed a framework for SER that integrates an autoencoder with a discriminative Restricted Boltzmann Machine (RBM)-based classifier. This approach considers unlabeled samples as an additional garbage class in the classification problem. The authors in Lian et al. (2020), Singkul and Woraratpanya (2022) and Falahzadeh et al. (2022) apply an end-to-end system to tackle SER tasks in which the neural network learns to automatically select the es- sential segments and discriminative features (Schuller, 2018). Han et al. (2014) employed the DNN model to take the speech segment’s features and extract them. Moreover, posterior probability statistics were used to produce the global features (utterance level characteristics). Finally,

Page 18:
Speech Communication 154 (2023) 102974  18 A. Hashem et al.  the extreme learning machine (ELM) classifier receives these utterance- level features as input. A method that uses an end-to-end system takes spectrograms as input and class labels as output (Fayek et al., 2017). Data augmentation is done by adding white Gaussian noise through- out the SER training process to increase the SER’s robustness   (Tiwari et al., 2020). Xia et al. (2019) incorporate two data augmentation techniques into our framework: one is a VTLP technique that lessens the impact of speaker changes on model performance, and the other is an NI technique that increases the resilience of FI-Net. Data augmen- tation is employed to augment the original signal data with additional features (Tariq et al., 2019). Fayek et al. (2015) used deep hierarchical architectures, data augmentation, and regularization with a DNN for SER. To enhance the SER, Yi and Mak (2020) suggested an adversarial data augmentation network (ADAN) that consists of a GAN, an auxiliary classifier, and an autoencoder feature selection. Lately, Ye et al. (2022) proposed a new method to capture fea- tures of emotions over different timescales. This approach is called Temporal-aware bi-directional Multi-scale Network TIM-Net. It looks at features from different timescales and combines them to understand emotions better. This architecture uses MFCC as input evaluated on six benchmarks SER corpora and achieved recent state-of-the-art results for speech emotion recognition on the RAVDESS, SAVEE, EmoDB, CASIA, and EMOVO datasets with 92.08%, 87.71%, 95.70%,94.67%, and 92.00% respectively. The extensive literature on DL techniques is investigated, including the databases utilized, the number of emotions identified, the type of feature set used, the type of DL classifiers employed, and the average accuracy attained. Table 11 summarizes the critical evaluation of the literature study of DL techniques for emotion recognition. The literature demonstrates that there is no one best way to categorize emotions. Therefore each classifier has its advantages and limitations.  8.3. Transfer learning  Instead of building DL architecture from scratch, transfer learning was also used in emotion recognition, as deep learning requires many speech-emotion corpora. During the problem of the lack of existing speech emotion corpora and to reduce learning time, there is a tendency to use spectrogram images as input for transfer learning and formulate it as an image classification task (Swain et al., 2022). Previous studies utilized the pre-trained convolutional neural networks, i.e., VGG (Si- monyan and Zisserman, 2014), and ResNet (He et al., 2016). Transfer learning can solve domain mismatches and adaptation among different data types to enhance the performance of SER (Deng et al., 2013). As a second stage, VGG was used to recognize emotion from a com- bination of features (Landry et al., 2020). The pre-trained model for ex- tracting features like (Huang et al., 2019) used two ResNet models, each extracting the features of speech and non-speech. Some authors attempt to enhance the performance using ResNet, such as Singkul and Worarat- panya (2022). They proposed a verify-to-classify framework called ResSE based on residual learning (ResNet) with squeeze-excitation (SE) block for generalized SER performance. A new pre-trained model was produced called PyResNet, an improved model of ResNet based on ResNet50, ResNet101, or ResNet152. A comparison experiment was done between VGG, ResNet, and the proposed PyResNet. The result shows the effectiveness of PyResNet and gives better results than VGG and ResNet on the LSSED dataset (Fan et al., 2021). Recently there has been a movement for utilizing Wav2vec in speech emotion recognition (Wongpatikaseree et al., 2022). In 2020 Wav2vec2 was designed by Facebook to introduce end-to-end deep learning that serves speech recognition tasks. The concept of Wav2vec is based on CNN, which accepts raw audio (1D sequence of numbers) as input under a self-supervised setting. CNN learns raw waveforms by performing a quantization representation. A transformer used such (self-attention) to learn the contextual representation. The transformer masks some vectors and then predicts their value as an output of the transformer. Contrastive loss is the loss function used. The model can use representative feature spaces to determine the relationship between speech features and emotion labels. As a result of utilizing contrastive loss between quantized feature encoding and context representation, the representative feature spaces are generalized (Baevski et al., 2020). Baevski et al. (2020) investigate several methods for extracting and modeling features from pre-trained Wav2vec 2.0 models for SER and suggest combining the layers’ output using trainable weights that were learned together with the downstream model. Zou et al. (2022) propose an end-to-end SER system by taking advantage of multi-level acoustic. information   and   designing   a   co-attention   module.   They   used Wav2vec 2.0 to extract features from the raw waveform. The study of Keesing et al. (2021) demonstrated that the feature set extracted from Wav2vec is one of the most effective techniques for SER tasks. Therefore, using Wav2vec in the SER task shows a promising result and may be a new area for research to investigate in detail.  8.4. Vocal bursts  As humans, the expression of emotions can be done in various ways. For example, we can use verbal like, speech, and nonverbal like pause or silence on sound. Non-verbal sounds, also called vocal bursts, are the sounds humans make to reflect emotional states, such as stutters, laughter, breath, and sighs. Vocal bursts are an often-overlooked factor of speech emotion recognition; however, they are essential to human vocal communication (Buhl, 2023). Available speech emotion data, primarily natural ones, contain these vocal bursts, as they can help represent human emotions through them. Recent studies examine the effectiveness of using non-verbal vocalizations to recognize speech emotions. Their results showed that these non-verbal vocalizations provided useful information that could not be obtained by lexical or acoustic features alone (Huang et al., 2019). Atmaja and Akagi (2020) claimed that when the silence is removed, it can slightly affect how smooth the speech sounds. Therefore, gener- ating the context artificially may not produce the same sound quality as if the context was created naturally. They use silence detected in audio recordings to recognize emotions in speech. The method involves measuring the silence in the audio, referred to as the Silence Feature, and comparing it to other data points, such as the mean and standard deviation of acoustic features in the audio. This comparison determines how effectively the Silence Feature recognizes emotions in speech. The comparison is measured using the Concordance Correlation Coefficient(CCC), Which is a way of estimating the agreement between multiple data points. The study conducts on the IEMOCAP dataset to study the impact of silent features on dimensional SER. The features used in this study are the GeMAPS feature set and mean std of GeMAPS (mean+std) and (mean+std+silence) features. They chose to use an LSTM-based dimension SER system due to its simple design and effi- cient use of hardware (specifically CuDNN). The best CCC score is when they use meant+std+silence. Huang et al. (2019) studied the effects of non-speech and non- vocal features in the SER task. They attempted to take advantage of the presence of the vocal burst by identifying vocal bursts separately from speech signals. They proposed an approach to recognizing emo- tions in spontaneous speech and segmenting the speech into prosodic phrases, nonverbal intervals, and silence sections. Two models were employed to extract features from each segment: a CNN-based model to extract emotion features and a sound feature model. Finally, these features were combined and used as input for an LSTM-based sequence- to-sequence model, which outputs the emotional sequence. The ex- periment’s results proved that nonverbal speech intervals improved emotion recognition performance. Later Hsu et al. (2021) improved the previous study that considered nonverbal vocalization in conversations depicting emotions of the same idea (Huang et al., 2019). Two ResNet models were trained to extract emotion features from verbal segment emotion-labeled verbal audio

Page 22:
Speech Communication 154 (2023) 102974  22 A. Hashem et al.  root mean square error (RMSE), and Pearson’s correlation coefficient (PR) (Abdelwahab and Busso, 2018). CCC is based on RMSE and PR; therefore, it is a more appropriate metric where an assessment of the correlation between predicted and actual values in the ML model is measured on a scale of [ − 1, 1]. A score of 0 indicates no correlation, while 1 signifies perfect agreement and   − 1 signifies perfect negative agreement (Fan et al., 2021).  11. Discussion  This section focuses on the research outcomes related to particular issues in the field of SER, along with recent studies conducted on the subject. This section covers the research questions mentioned in Section 2. For this SLR, 96 papers were chosen from three distinct sources based on the details provided in Table 2. While specific papers are related to emotional databases or speech features, the majority of them are associated with classifiers. The goal of this study is described through the research questions specified in Table 1 and overcome the weaknesses observed in the related surveys mentioned in Section 3. All specified research questions and their outcomes are addressed below.  11.1. RQ1: What are the existing freely available datasets for speech emotion recognition?  From the literature on speech databases, some databases might be available to the public, others not. Some need an academic li- cense to have access and can be used. Researchers have used different databases among the 96 research papers in this study. Databases used in the examined papers have four types. First, papers used free public databases, which are also benchmark datasets. Second, papers used a privet database. Third, a combination of multiple datasets, whether free or not. Finally, some papers created their own database. Most of the examined papers used freely available public databases. Most of the SER models were evaluated on the EMODB database 30%, followed by IEMOCAP 25%, SAVEE 12%, and RAVDESS 10% (SER models are presented in Tables 9 and 11). Information on the databases that are commonly used is available in Table 12, whereas Fig. 11 shows the frequency of use of the data by researchers in the selected papers. The researchers chose these datasets because the quality and the size are relevant and reasonable. There are several free databases available, though few researchers turn to them. We think the reason is due to database problems, which require more work in the preprocessing. Data quality and balance are also essential factors. A list of existing freely available datasets is given in Table 13.  11.2. RQ2: what are the features used to detect emotions from speech?  Studies try to examine different acoustic features by utilizing a single feature or combining more than one feature to discriminative the emotions correctly. Some of the ML papers lean on feature selection methods, as discussed in Section 7, whereas DL papers automatically learn features such as images or the wave itself. It is discovered through the literature that many features have been used and studied by re- searchers. Speech features derived from speech for SER can include local, global, or both features. Four categories are examined: Spectral features, prosodic features, Teager energy operator (TEO), and voice quality features. Table 14 lists the speech features, along with their purpose and approaches. However, most features used in the literature are spectral features, specifically the spectrogram followed by MFCC alone or with other feature types. TEO features were the most lacking feature found, which might be because they related to stress detection. The features utilized according to the approximate number of papers are illustrated in Table 15. There is no golden set or set of features that ensure accurate classification results that reflect the different emotions. Most existing studies are based on an experimental basis or use the rule of thumb (Swain et al., 2018).  Fig. 11.   The distribution of papers based on the speech databases utilized, along with their corresponding percentages (a) highest four databases used, (b) all databases used.  11.3. RQ3 what are the learning models used to detect emotion from speech?  To enhance the recognition result, models try to capture the com- plex features that need a complex architecture and cause a high compu- tational complexity. Deep CNN architectures have improved accuracy, but the cost complexity has also increased. RNN and LSTM are proper for training sequential data but are more computationally complex and difficult to train effectively. Therefore, some studies take into consideration reducing computational complexity (Liu et al., 2021; Kwon et al., 2021). The attention technique is used to reduce com- putational complexity by looking for specific components of a signal instead of the whole signal (Guo et al., 2022; Mirsamadi et al., 2017). Most SER systems aim to remove emotional ambiguity by creating an appropriate model that instructs to learn discriminative features with reduced intra-class variance and increased inter-class variance. Researchers have used different models to learn salient features and classify emotions appropriately. Most of the examen papers used DL approaches with different architectures, as Tables 9 and 11 represent. In the ML approaches, SVM was used extensively due to the advantages, as discussed in Table 8, whereas most DL approaches used CNN in the architecture. The reason is the majority of spectral features, as it has been previously established that most papers have used spectral features converted into visual images. In these cases, CNN is the best to deal with.  11.4. RQ4: What are the metrics used to measure the performance of SER models?  They have selected different metrics depending on what the authors want to measure in the study. However, in SER, select metrics also depend on the modality labels of the dataset, as covered in Section 10. From the examined paper, most of them used weighted accuracy and unweighted accuracy along with a confusion matrix that reflects the

Page 23:
Speech Communication 154 (2023) 102974  23 A. Hashem et al.  Table 12  The distribution of papers based on the SER databases used. Database   Number of papers EmoDB   Ye et al. (2022), Mansouri et al. (2022), Jiang et al. (2019), Falahzadeh et al. (2022), Li et al. (2015), Ye et al. (2022), Keesing et al. (2021), Özseven (2019), Singkul and Woraratpanya (2022), Yildirim et al. (2021), Ntalampiras (2021), Chen et al. (2020), Li et al. (2021a), Liu et al. (2021), Mao et al. (2017), Xu et al. (2022), Guo et al. (2022), Issa et al. (2020), Langari et al. (2020), Kwon et al. (2021), Wang et al. (2020), Daneshfar et al. (2020), Ancilin and Milton (2021), Li and Akagi (2019), Henríquez et al. (2014), Zhao et al. (2019b), Li et al. (2021b), Thirumuru et al. (2022), Zayene et al. (2020), Guo et al. (2018), Stolar et al. (2018), Sinith et al. (2015), Zhou et al. (2016), Xia et al. (2019), Li et al. (2013), Xia et al. (2020), Tiwari et al. (2020), Aftab et al. (2022), Stolar et al. (2017), Sugan et al. (2018), Lim et al. (2016), Deng et al. (2017b), Zão et al. (2014), Yi and Mak (2020) and Deng et al. (2017a) IEMOCAP   Mansouri et al. (2022), Han et al. (2014), Li et al. (2015), Dai et al. (2019), Ye et al. (2022), Keesing et al. (2021), Kwon (2019), Wang et al. (2022), Pepino et al. (2021), Atmaja and Akagi (2020), Yildirim et al. (2021), Chen and Huang (2021), Zhao et al. (2021b), Liu et al. (2022, 2021), Li et al. (2021a), Xu et al. (2022), Guo et al. (2022), Issa et al. (2020), Kwon et al. (2021), Daneshfar et al. (2020), Zhao et al. (2021a), Yao et al. (2020), Fayek et al. (2017), Zhao et al. (2019b), Li et al. (2021b), Thirumuru et al. (2022), Zayene et al. (2020), Mirsamadi et al. (2017), Xia et al. (2020), Tiwari et al. (2020), Aftab et al. (2022), Luo et al. (2017), Zou et al. (2022), Li et al. (2021c), Yi and Mak (2020) and Zhao et al. (2019a) SAVEE   Li et al. (2015), Ye et al. (2022), Keesing et al. (2021), Wang et al. (2022), Özseven (2019), Atila and Şengür (2021), Langari et al. (2020), Manohar and Logashanmugam (2022), Daneshfar et al. (2020), Ancilin and Milton (2021), Li and Akagi (2019), Li et al. (2021b), Zayene et al. (2020), Qayyum et al. (2019), Sinith et al. (2015), Sugan et al. (2018), Wani et al. (2020) and Fayek et al. (2015) RAVDESS   Chalapathi et al. (2022), Ye et al. (2022), Keesing et al. (2021), Kwon (2019), Pepino et al. (2021), Singkul and Woraratpanya (2022), Atila and Şengür (2021), Liu et al. (2021), Issa et al. (2020), Kwon et al. (2021), Manohar and Logashanmugam (2022), Ancilin and Milton (2021), Lee et al. (2019), Sefara and Mokgonyane (2020) and Tariq et al. (2019)  Table 13  The existing free datasets that are available for speech emotion recognition. Freely database used   Ref. CHEAVD 2.0 DB   Li et al. (2017) SSPNet DB   Joshi et al. (2020) Malayalam DB   Sinith et al. (2015) Serbian emotional speech DB   Jovicic et al. (2004) LSSED DB   Fan et al. (2021) Fujitsu DB   Elbarougy (2019) SmartKom DB   Schiel et al. (2002) TEDLIUM2 DB   Rousseau et al. (2014) Urdo DB   Latif et al. (2018) VENEC DB   Cowen et al. (2019) AESDD   Vryzas et al. (2018) CDESD DB   Jing et al. (2018)  performance accuracy of the model on each emotion state. In addition, standard metrics such as precision and F1 score are used. Depending on the dimensional label, one paper used the concordance correlation coefficient (CCC) for measurement.  12. Limitations of this review  This survey considered only speech emotion recognition that used only speech audio as an input where only acoustic features were covered. However, many studies nowadays use multi-modal (different modes/modalities) approaches (Xu et al., 2020; Aslan et al., 2017; Kumar et al., 2022) , which uses different features by adding another input, such as an image, transcripts, or both, to enhance the recognition result., which uses different features by adding another input, such as an image, transcripts, or both, to improve the recognition result. Furthermore, it does not cover the effect of cross-lingual, the different languages, and cultures on SER, where several studies cover cross- language. Many papers claimed that SER is language-independent as emotions are a paralinguistic feature (Saad et al., 2021; Shaukat and Chen, 2010). Hence emotions are communicative information that is non-linguistic and non-verbal but coded in the speech signal. Current research focuses on various issues, including some that still need to be solved. These include features, classifiers, cross-lingual emotion recognition, and real-time emotion recognition, which could be the focus of future SER research’s forthcoming studies.  13. Future direction  This research work aims to explore the previous effort that has been made in the recognition of emotions from speech. The existing approaches of SER are reviewed and analyzed in many aspects. Consid- ering the significant amount of time and effort given to SER research, it opens new paths and raises some challenges. Several future directions are possible for research in SER. This section provides the following future guidance based on the review’s findings:  •   The problem of mismatched environments and the ability to adapt between differentiation of speech in terms of languages/cultures and type of data. Applying a model trained using data from one domain to another lead to performance degradation. Similarly, when building a model and training it on acted type dataset, the expected performance will suffer when applying the model in a real-life application. One solution that might be considered is using transfer learning. The few attempts that used transfer learning dealt well with the domain mismatched and improved the performance of SER (Deng et al., 2013). Therefore, domain generalization occurs when there are noticeably significant do- main shifts despite having corpora expressing the same emotions across languages and speakers. For SER, the model must be generalized to the unseen domain.  •   The use of unlabeled speech utterances, where it is known that speech emotion databases are lacking and suffer from data, it is hard to collect a large-scale database with the label for the SER system. Therefore, utilizing an unlabeled dataset to tackle this issue is significant. Semi-supervised learning has been used for

Page 24:
Speech Communication 154 (2023) 102974  24 A. Hashem et al.  Table 14  Literature summary of features used in speech emotion recognition. Speech features types   Features   Purpose and approach   Ref. Spectral features   MFCC, LPCC, LFPC, GFCC   ∙   The spectral features are obtained using vocal tract features.  ∙   The Fourier transform can be used to extract spectral features.  ∙   More speech information is contained in the lower number of MFCCs. Fleischer et al. (2015), Brigham and Morrow (1967) and Heinzel et al. (2002) Prosodic feature   Fundamental frequency, Pitch, Energy, Duration, Frame level features  ∙   Prosodic features are considered a useful correlate of human emotions.  ∙   Sentences’ prosodic properties at the global and local levels are extracted.  ∙   Prosodic features extracted are long-term features that are taken from speech.  ∙   Prosodic features considerably rely on fundamental frequency, duration, and energy. Schroder and Cowie (2006), Daneshfar et al. (2020) and Li and Akagi (2019) Teager energy operator   TEO critical band auto envelope, TEO auto envelope, TEO frequency modulation variation  ∙   It has been observed that in a stressful situation,the critical band and frequency change due to the distribution of harmonics.  ∙   When there are no stressful situations, the airflow is altered during speech generation. Teager and Teager (1983) and Teager (1980) Voice quality features   Bandwidth, Glottal parameters, Harmonics to noise ratio, Shimmer, Jitter  ∙   Voice quality features are a crucial aspect of speaker identification and speech processing.  ∙   The emotional content and voice quality of speech has contrasting interrelationships. Teixeira et al. (2013) and Fernandes et al. (2018)  Table 15  The features utilized according to the examined papers. Features extracted   Number of papers Spectral features   Alam et al. (2013), Mansouri et al. (2022), Jiang et al. (2019), Jain et al. (2020), Elbarougy (2019), Gilke et al. (2012), Falahzadeh et al. (2022), Li et al. (2015), Chaudhari and Alex (2016), Dai et al. (2019), Sheikhan et al. (2013), Ye et al. (2022), Kwon (2019), Atila and Şengür (2021), Chen and Huang (2021), Zhao et al. (2021b), Ntalampiras (2021), Chen et al. (2020), Li et al. (2021a), Guo et al. (2022), Issa et al. (2020), Manohar and Logashanmugam (2022), Li and Akagi (2019) and Fayek et al. (2017) Prosodic feature   Jain et al. (2020), Li et al. (2015, 2021a), Li and Akagi (2019), Fayek et al. (2017), Sinith et al. (2015), Zhou et al. (2016), Xia et al. (2019), Tiwari et al. (2020), Mirsamadi et al. (2017), Deng et al. (2017b), Daneshfar et al. (2020), Chen et al. (2020), Jing et al. (2018), Sheikhan et al. (2013), Fayek et al. (2017), Sinith et al. (2015), Han et al. (2014) and Gilke et al. (2012) TEO   Zão et al. (2014) Voice quality   Li et al. (2015), Mirsamadi et al. (2017), Chen et al. (2020), Tarunika et al. (2018) and Chalapathi et al. (2022) Automatic learned   Zhou et al. (2016), Wang and Han (2019), Qayyum et al. (2019), Wang and Tashev (2017) and Mirsamadi et al. (2017)  this, where successful exploration of learning from speech using numerous unlabeled data has been conducted (Luo et al., 2017).  •   The majority of the existing studies used categorical datasets, which according to Ekman et al. (2013) and despite the different cultures, indicate six primary emotions happy, sad, fear, anger, surprise, and disgust. However, using dimensional representations of emotion based on 2 or 3 dimensions (valence, arousal, domi- nant) can enhance the capture of the complex emotion, which is helpful for SER in real-life applications.  •   Most of the current studies use popular databases. However, as shown in Table 12, some databases are freely accessible, but only a few studies covered them. An attempt to improve and use these databases to recognize emotions through speech sheds light on some new challenges that need to be explored by researchers as well as increase the number of SER databases.  •   There are still many scopes for researchers to discover the features that indicate robustness in SER. While spectral features are most used and achieve a high recognition accuracy, researchers can examine the usefulness of other features, such as TEO-based fea- tures in SER research, since they have not been much considered in the studies. Also, fusion techniques for different features have enhanced the recognition result, which aims to extract features from different domains to capture the salient features. Also, recent studies consider vocal burst features to enhance the recognition result for SER tasks.  •   An extensive study is needed on computational complexity that focuses on different techniques to reduce the computational com- plexity of features and learning models to be suitable for different machines with limited capacity.  14. Conclusions  This systematic literature review aimed to examine and analyze the existing papers in SER by selecting them according to specific criteria to answer the research questions. Furthermore, it offered a comprehensive analysis of SER methodologies, covering specific research concerns and recent studies. In this SLR, 96 papers were selected from three different databases according to the details shown in Table 2. All existing SER surveys were covered and compared to this survey in various aspects, as discussed in Table 3. The papers from 2012 to 2022 covered and discussed different databases with the modality of labels used. Data processing methods, types of extracted features, and feature selection methods have been explained in depth. The classifiers in used SER are covered for traditional ML, DL approaches, and transfer learning.

Page 9:
Speech Communication 154 (2023) 102974  9 A. Hashem et al.  Table 5   ( continued ). Data modality Ref.   Language   Type of database Type of file Number of emotions Number of speakers Number of samples Public Access Brief Description VAM (Grimm et al., 2008) German   Natural   Audio, Visual 3   –   12 h   Yes   Database created in German language and contains 47 Utterers from talk-show, 947 utterances. 3 dimensions Valence, activation, and dominance. Dimen- sional MSP-Podcast (Lotfian and Busso, 2017) English   Natural   Audio   3   60 (30F, 30M) 27 h   Yes   Database with balanced emotional content, created in English language from podcast audio and contains 3 dimensions Valence, activation, and dominance. SAMAINE (McKeown et al., 2011) English Greek Hebrew Natural   Audio, Visual 3   150 S   –   Yes   The database is based on recording a dialogue between humans/users and artificially intelligent agents/operators. The emotion is tagged into five dimensions: Valence, activation, power, expectation, the overall emotional intensity. Both   IEMOCAP (Busso et al., 2008) English   Actor based Audio, Visual 5   10 (5M, 5F) 1150   Yes   Database created in English language by 10 actors consisting of audio and video. The annotation of the emotions is neutral, happiness, sadness, anger, surprise, fear, disgust, frustration, excited, and 3 dimensions. Available with license. RECOLA (Ringeval et al., 2013) French   Natural   Audio, Visual 2   46 (19M, 27F) 7 h   Yes   Database created different multimodal data like audio, video, ECG, it is in French language. The annotators measured emotion on two dimensions and social behavior labels on five dimensions agreement, dominance, engagement, performance, rapport.  Fig. 5.   Processing techniques of signal for speech emotion recognition.  the segmentation of the signals with an overlapping. The benefit of framing the signals is to obtain them in a fixed length, which is suitable for the classifier while keeping the emotional knowledge in speech. Some feature extraction techniques like the use of Discrete Fourier Transform (DFT), where continuous speech signals may control the use of processing (Heinzel et al., 2002).  5.2. Windowing  After completing the framing phase, the second phase is to apply the window function to the frames. To enhance a sampled signal’s spectral properties, use smoothing windows where it can be used to reduce the discontinuities of truncated waveforms, hence lowering spectral leak- age while doing Fourier or spectral analysis on finite-length data (Beigi, 2011). The amplitude of the discontinuity determines how much spec- tral leakage occurs. Spectral leakage grows as the discontinuity gets more expansive, and vice versa. Smoothing windows function as nar- rowband lowpass filters by reducing the magnitude of discontinuities at each period’s boundaries (Heinzel et al., 2002). When windowing a signal, the time record is multiplied by a smoothing window of limited length, whose amplitude gradually decreases to zero at the window’s edges (Hamid, 2018). When windowing a signal, the time record is multiplied by a smoothing window of limited length, whose amplitude gradually decreases to zero at the window’s edges. The number of samples determines the smoothing window’s length or time interval. Convolution in the frequency domain is the same as multiplication in the time domain. As a result, the windowed signal’s spectrum is a con- volution of the original signal’s spectrum and the smoothing window’s spectrum. In addition to altering the spectrum, windowing modifies the time domain form of the signal (Prabhu, 2014). A Hamming window is commonly utilized, as shown in Eq. (1), where the window size is   𝑀  for the frame   𝑤 ( 𝑛 ) .  𝑤 ( 𝑛 ) = 0 . 54 − 0 . 46   𝑐𝑜𝑠   (   2 𝜋𝑛 𝑀   − 1 )   (1)  5.2.1. Time windows  SER is the task of automatically identifying emotions from audio speech signals, Where the time window refers to the duration of speech segments used for feature extraction and emotion classification. It involves dividing the continuous speech signal into smaller segments to analyze emotional expressions’ acoustic properties and temporal patterns. The time window in SER is essential because emotions are dynamic and evolve. Also, the effectiveness of features depends upon the duration of the time window chosen. Different emotional cues may

Page 16:
Speech Communication 154 (2023) 102974  16 A. Hashem et al.  Table 8  Summary of traditional classifiers used in speech emotion recognition. Classifier   Advantages   Disadvantages GMM (Schuller, 2011; Alam et al., 2013; Bie et al., 2013; Zão et al., 2014).  ∙   Efficient the Computation.  ∙   Easy to be executed.  ∙   Proper for extracting global features (HSF).  ∙   Powerful probabilistic framework.  ∙   Not suited for modeling the temporal structure.  ∙   Need to collect all parameters of the model before beginning. HMM (Chaudhari and Alex, 2016; Schuller et al., 2003; Lin and Wei, 2005).  ∙   Does not depend on the text.  ∙   suited for modeling the temporal structure.  ∙   Increase in computational complexity.  ∙   Depending on the initialization of the model parameters. SVM (Seehapoch and Wongthanavasu, 2013; Jain et al., 2020; Yang et al., 2017).  ∙   Suitable for small datasets.  ∙   Generalized better.  ∙   Utilized for classification or regression problems.  ∙   Sensitive to the choice of kernel.  ∙   Computationally expensive when the dataset is large.  Table 9  Summary of the literature used traditional techniques for speech emotion recognition. Database   Emotions recognized   Features used   Classifiers   Accuracy   Ref. AIBO   Angry, Emphatic, Neutral, Positive, and Rest. AMCC features   GMM   44.50% 42.58%   Alam et al. (2013) LDC and UGA   Happy, Sad, Angry, Fear.   Pitch, MFCC, LPCC, Energy, speech rate SVM   85.085%   Jain et al. (2020) FARSDAT   Happiness, anger, neutral   MFCCs, LE and their first and second derivatives, formant- and pitch-related features SVM   76.3%   Sheikhan et al. (2013) EMO-DB, eNTERFACE05, EMOVO, SAVEE Anger, Disgust, Anxiety/Fear, Happiness, Sadness, Boredom, Surprise, Neutral A Feature selection method used   SVM, MLP, K-NN   84.62%, 69.23% 60.40%, 72.39% Özseven (2019) EMO-DB, IEMOCAP   Angry, Boredom, Disgust, Fear, Happy, Sad, Neutral. INTER-SPEECH 2010 paralinguistic SVM, KNN, TreeBagger 87.66% 69.30%   Yildirim et al. (2021) Emo-DB, IEMOCAP   Neutral, sadness, happy, anger   Mel-Generalized Delta Cepstral Coefficients (MGDCC), dynamic relative phas (DRP), Relative Phase (RP), magnitude spectrogram KELM   94.02% 58.07%   Guo et al. (2022) IEMOCAP, MSP-Improv, RAVDESS, DEMoS, EmoDB, ShEMO Happy, Angry, Sad, Neutral. Happy, Angry, Sad, Neutral. Calm, Angry, neutral, Fear, Happy, Sad, Disgust, Surprise. Angry, neutral, Fear, Happy, Sad, Disgust, Surprise. Angry, Disgust, Fear, Happy, Natural, Surprise, Bored. Angry, Fear, Natural, Surprise, Happy, Sad. Phonemic features (STFT)   RandomForest, SVM-RBF, KNN, MLP 62.66%, 52.48%, 64.32%, 48.79%, 73.46%, 70.53% Liu et al. (2021) CASIA, EmoDB   Angry, Fear, Happy, Neutral, Sad, Surprise. Extracted features using openSMILE toolkit Random forest   81.75%, 77.94%   Chen et al. (2020) EMO-DB, SAVEE, PDREC Sad, Neutral, Happy, Fear, Disgust, Bored, Angry. Sad, Neutral, Happy, Fear, Disgust, Surprise, Angry. Sad, Neutral, Happy, Fear, Disgust, Surprise, Bored, Angry. Time–Frequency features   SVM   97.57%, 80%, 91.46% Langari et al. (2020)  temporal features. Table 10 summarizes the DL approaches utilized for SER with a brief illustration. The current research studies use DL models where the learning process can be done using handcrafted features, spectrograms, or the signal wave. In emotion recognition tasks, CNN has been utilized in different architectures for SER systems (Lin and Wei, 2005; Chavhan et al., 2010; Teixeira et al., 2013; Fernandes et al., 2018; Teager and Teager, 1983; Teager, 1980; Kaiser, 1990; Zhou et al., 2001; Tang et al., 2018; Schuller et al., 2020; Freitag et al., 2017; Papakostas et al., 2017; Zheng et al., 2015). To evaluate the IEMOCAP, EMODB, eNTERFACE, and SAVEE datasets, Li et al. (2021b) proposed a CNN-based innovative spatial, temporal, and frequential network for emotion recognition obtained an average accuracy of 71.98%, 82.10, 75.60%, and 54.75%, respectively. To extract the prominent discriminative characteristics, Kwon (2019) proposed a deep stride CNN. This basic CNN architecture omitted the pooling layer and used strides to downsampling input feature maps. Using a more straightforward CNN structure, they cor- rectly addressed preprocessing with innovative adaptive thresholding for noise reduction and decreased computational complexity in IEMO- CAP and RAVDESS datasets. This step increased accuracy by 7.85% and 4.5%, respectively. Afterward, Wani et al. (2020) implemented the same approach and achieved 87.8% accuracy on the SAVEE speech emotion dataset. Issa et al. (2020) built a model by using DCNN tech- nique to recognize emotions from speech audio. The extracted features are input for 1D-CNN; however, this model gives low-performance results when used for large training data. To tackle this issue, Pepino et al. (2021) suggest combining features from frozen wav2vec2.0 with other handcrafted prosodic features. Then they fed them into a 1D-CNN for a deeper extraction, where the analysis of Keesing et al. (2021) demonstrated that one of the best feature extraction methods for SER

Page 21:
Speech Communication 154 (2023) 102974  21 A. Hashem et al.  Table 11   ( continued ). Database   Emotions recognized   Features used   Classifiers   Accuracy   Ref. TEDLIUM2   Happy, Angry, Sad.   Spectrogram   CNN   66.1%   Bertero and Fung (2017) IEMOCAP   Happy, Angry, Sad, Neutral.   Automatically learned   RNN   63.5%   Mirsamadi et al. (2017) Mandarin dataset   Happy, Sad, Anger, fear, Neutral, Surprise, Disgust.   Automatically learned   DNN based ELM   82.18%   Wang and Tashev (2017) EMO-DB   Anger, Happy, Sad, Fear, Disgust, Bored, Neutral. Spectrogram   FTAlexNet   76%   Stolar et al. (2017) IEMOCAP   Angry, Neutral, Sad, Excitement.   Spectrogram, F0, MFCC, energy, ZCR, voice probability Ensembling Neural Networks 59%   Luo et al. (2017) 2016 EMO-DB   Anger, Bored, Disgust, Anxiety, Happy, Sad, Neutral Automatically learned   DNN   65%   Zhou et al. (2016) EMO-DB   Neutral, Anger, Fear, Disgust, Sad, Bored, Happy. Spectrogram   CNN + LSTM   88.01%   Lim et al. (2016) 2015 eNTERFACE, SAVEE. Anger, Disgust, Fear, Happy, sad, Surprise. Anger, Disgust, Fear, Happy, sad, Surprise, Neutral. Spectrogram   DNN   60.53%, 59.7%   Fayek et al. (2015) 2014 LCD, EMO-DB, Polish emotional speech database Neutral, Fear, Anger.   Nonlinear Dynamic Features   ANN   80.75%, 75.4%, 72.28% Henríquez et al. (2014) 2013 NTERFACE’05, EMO-DB, Anger, Sad, Happy, Disgust, Fear, Surprise. Anger, Bored, Disgust, Fear, Happy, Sad, Neutral. MFCC   DNN-HMM   53.89%, 77.92%,   Li et al. (2013) Serbian emotional speech database Anger, Fear, Joy, Sad, Threat.   MFFC, Temporal Discrete Cosine Transform (TDCT) ANN   67.22%,   Popović et al. (2013)  some reasons covered in Section 4. Using small data in deep learning may generate overfitting due to the enormous number of parameters in deep learning. To tackle this problem, either augment the sample size through oversampling or apply transfer learning techniques. By using perturbation of the vocal tract length (VTLP) of the samples from the classes with lower representation in a database, oversampling can be accomplished. It is similar to augmentation, where the idea of the method is to rescale the original spectrogram (from 0.9 to 1.1) along the frequency axis and creates new samples as a result. When oversampling was utilized, accuracy increased by 4% (Etienne et al., 2018). Some researchers try to increase the number of data by using more than one data and combining them. Siegert et al. (2018) employed a similarity measure based on PCA to combine different databases. Another approach is transfer learning, where the underlying distribution features of unlabeled data are transferred to labeled data. Hence, a deep learning model can be constructed using a smaller amount of labeled data. Considering the small dataset, Boigne et al. (2020) utilized transfer learning of recognizing emotions. Another area for improvement is the natural dataset suffers from imbalanced data. For example, the natural data recording from sponta- neous conversation does not control the number of samples for each emotion class, which leads to the problem of biasing the emotion class that contains a more significant number of samples. Chatziagapi et al. (2019) proposed a GAN architecture for in-class spectrogram generation where it generates artificial samples to overcome the data imbalance issue for the task of SER and augment the minority emo- tional classes. Shih et al. (2017) during the weight update process, skewed weights were assigned to the samples, whereby the majority classes received lower weights, and the minority classes were allocated higher weights. Utterances with near-constant length, or short to the average length, are better for emotion recognition than long utterances since a shorter utterance maintains emotional expression throughout the shorter utter- ance. In contrast, long utterances can contain a mixture of emotions. Therefore, it is important to identify which parts of the utterance carry the emotion and focus the training and testing on them. Chang et al. (2018) found that the emotion in short-length utterances (1–3 s) was easier to recognize than in longer ones. To solve the problem of unequal utterance lengths in DL algorithms, two strategies for dealing with variable-length utterances in emotion recognition systems are either padding smaller utterances with zeros to reach the desired length or segmenting them into fixed-length segments. Both of these strategies require breaking down the long utterances into smaller parts. This makes it easier to recognize the emotion in each smaller section rather than trying to identify a single emotion in the entire utterance (Zhang et al., 2017).  10. Evaluation metrics  After selecting the dataset, extracting features, and choosing an al- gorithm for classification, evaluation metrics are needed to evaluate the model’s performance. In SER systems, the measurement used depends on the problem, whether classification or regression, as discussed in Section 4. Many previous studies used accuracy in the SER field to measure performance (Pandey et al., 2019; Hsu et al., 2021; Kwon, 2019; Huang et al., 2019). However, there is a problem as accuracy is suitable for use when the data is balanced. Nevertheless, having imbalanced data in SER systems is a problem (Li et al., 2020), such IEMOCAP data (Busso et al., 2008). Therefore, using accuracy alone may not be suitable. Therefore, unweighted accuracy (UW) is used along with weighted accuracy (WA) (Zou et al., 2022; Seehapoch and Wongthanavasu, 2013). Weighted accuracy assigns weight to each class based on their sample number. In contrast, unweighted accuracy gives the same weight to all classes regardless of the number of correspond- ing samples, which is suitable for imbalanced datasets (Gupta et al., 2020). Different metrics have been used, for instance, recall, precision, and F1-score (Chatziagapi et al., 2019; Singkul and Woraratpanya, 2022). Also, many studies used the confusion matrix for categorical emotion analysis, describing how the model performs on each emo- tional class (Atmaja and Akagi, 2020; Hsu et al., 2021). To measure the performance of the continuous emotions in the regression problem model, mainly used the concordance correlation coefficient (CCC),

Page 8:
Speech Communication 154 (2023) 102974  8 A. Hashem et al.  Table 5  Literature summary of data modality and emotion speech database. Data modality Ref.   Language   Type of database Type of file Number of emotions Number of speakers Number of samples Public Access Brief Description EMO-DB (Burkhardt et al., 2005) German   Actor based audio   7   10 (5M, 5F) 800   Yes   Database created in german language by 10 actors using 10 utterances, 5 short and 5 long. The labels of the emotions are anger, happiness, sadness, neutral, disgust, boredom, and fear. DES (Engberg et al., 1997) Danish   Actor based audio   5   4 (2M, 2F) 210   Yes   Database created in Denmark language by 4 actors with an average length of 2.7s. The labels of the emotions are anger, sadness, happiness, surprise, and neutral. Categorical   AIBO (Steidl, 2009)   German   Natural   Audio   3   51 children 9 h   No   Database created in the German language. Includes 51 kids speaking to robot dog Aibo, 9h of speech. The labels of the emotions are anger, bored, emphatic, helpless, joyful, motherese, and neutral. It is Accessible on price. eNTERFACE (Martin et al., 2006) English   Actor based Audio, Visual 6   42 (34M, 8F) 1287   Yes   It is a multimedia database. The database’s goal is to support audio–visual speech recognition and SER. It is freely available for research purposes. The labels emotion are anger, happiness, surprise, fear, disgust, and sadness are included. SAVEE (Haq and Jackson, 2011) British English Actor based Audio, Visual 7   4 M   480   Yes   It is completely free to use for research purposes. The included labels are anger, happiness, neutral, surprise, fear, disgust, and sadness. CREMA-D (Cao et al., 2014) English   Actor based Audio, Visual 6   91 (48M, 43F) 7442   Yes   The average length is 2.5s. Anger, happiness, sadness, fear, neutrality, and disgust are all annotated emotions. CHEAVD (Li et al., 2017) Chinese   Natural   Audio, Visual 26   238 S   2322   Yes   The database extracts from films, television, and talk shows, including 140 min of emotional utterances. It provides emotional levels and suppressed/fake emotional labels. There are 26 emotions and speakers ranging in age from child to elder. It is freely available for research purposes. CASIA (Bao et al., 2014) Chinese   Natural   Audio, Visual 24   219 S   12,000   No   The total number of emotions in this database is 24, represented by 2 h of spontaneous emotional segments taken from 219 speakers in 25 films, one television series, and 22 talk shows. Categorical   IIT-H TEMD (Rambabu et al., 2020) Telugu   Semi- natural Audio   14   38 (20M, 18F) 2450   Yes   The Database emotions, including happy, angry, sad, neutral, surprised, fearful, disgusted, sarcastic, frustrated, relaxed, worried, shy, Excited, and shouted where; it is collected by designed drama situations from actors and non-actors for SER purposes. It is available Publicly. TESS (Pichora-Fuller and Dupuis, 2020) English   Actor based Audio   8   2 F   2800   Yes   Two actresses aged 26 and 64 created a simulated database that included Anger, happiness, surprise, fear, disgust, sadness, neutrality, and pleasantness. RAVDESS (Livingstone and Russo, 2018) American English Actor based Audio, Visual 7   24 (12M, 12F) 7356   Yes   The database has public access and is presented as speech and music where the emotions are happy, angry, neutral, surprised, fearful, disgusted, and calm. It can be used as audio or visual, or both. AESDD (Vryzas et al., 2018) Greek   Actor based Audio   5   2M, 3F   500   Yes   The dataset is a Greek with acted speech, created with five actors aged 25 to 30, including both genders with 500 samples, covering five emotions: happiness, sadness, anger, fear, and disgust. EMOVO (Costantini et al., 2014) Italian   Actor based Audio   7   6 (3M, 3F) 588   Yes   This dataset is the first Italian emotional corpus where it is available publically. It includes seven emotions disgust, joy, fear, surprise, sad, joy, and neutral. ( continued on next page )  conversion (ADC) into small frames. One problem on the SER dataset has varying sample lengths (Li et al., 2020). Therefore, the audio wave signal is segmented into fixed lengths sections. The segmentation approximates the local features from the close to a stationary state. As known, speech signals are non-stationary; nevertheless, they remain constant for a short interval (Ozseven, 2018). Therefore, these frames should be related to each other by overlapping where most studies overlapped frames signals between 30% and 50%. Fig. 6 illustrates

Page 19:
Speech Communication 154 (2023) 102974  19 A. Hashem et al.  Table 11  Summary of literature review of the deep learning techniques for speech emotion recognition. Database   Emotions recognized   Features used   Classifiers   Accuracy   Ref. 2022 IEMOCAP, SAVEE, CHEAVD, CASIA. Happy, Angry, Sad, Neutral. Anger, Disgust, Fear, Happy, Neutral, Sad, Surprise. Anger, Happy, Sad, Worry, Anxiety, Surprise, Disgust, Neutral. Angry, Happy, Sad, Surprise, Neutral Handcraft + DNN features   Local Attention + RNN 72.3%, 81.8%, 55.7%, 63.3% Wang et al. (2022) IEMOCAP   Happy, Angry, Sad, Neutral.   MFCC, Spectrograms, Wav2vec features Co-attention mechanism 71.05%   Zou et al. (2022) EMO-DB, IEMOCAP, eNTERFACE05 Neutral, Sad, Happy, Anger. Happy, Angry, Sad, Neutral. Anger, Disgust, Anxiety/Fear, Happy, Sad, Bored, Surprise, Neutral. Spectrograms   DCNN   95.14%, 74.23%, 89.46% Mansouri et al. (2022) IEMOCAP, eNTERFACE05 Happy, Angry, Sad, Neutral. Happy, Sad, Bored, Surprise, Neutral Chaogram images   DCNN   96.04%, 90.40%   Falahzadeh et al. (2022) EMO-DB, IEMOCAP   Neutral, Sad, Happy, Anger. Happy, Angry, Sad, Neutral. Spectrogram   +   eGeMAPS   +  MFCC Hybrid DNN   91.25%, 73.42%   Xu et al. (2022) Speech-emotion- recognition-master, SAVEE, CREMA-D, Ravdess. Angry, Happy, Neutral, Sad. Anger, Disgust, Fear, Happy, Sad, Surprise, Neutral. Anger, Disgust, Fear, Happy, Neutral, Sad. Calm, Happy, Sad, Angry, Fear, Surprise, Disgust. MFCC, Mel-spectrogram, Tonal power, Spectral flux DNN + RNN   97.4%, 95.7%, 97.5%, 95.6% Manohar and Logashanmugam (2022) SHEMO   Anger, Fear, Happy, Sad, Surprise.   MFCC   1D-CNN   74%   Siadat et al. (2022) IEMOCAP, EMO-DB   Happy, Sad, Angry, Neutral. Happy, Sad, Angry, Neutral, Fear, Disgust, Bored. MFCC   CNN   79.87%, 94.21%   Aftab et al. (2022) 2021 RAVDESS, RML, SAVEE. Angry, Calm, Disgusted, Fear, Happy, Neutral, Sad, Surprise. Spectrogram, cochleagram, MFCC 3D CNN-LSTM   96.18%, 93.2%, 97, 5% 95% Atila and Şengür (2021) IEMOCAP   Happy, Sad, Angry, Natural.   Mel-spectrogram, MFCC&deltas Dual attention- BLSTM model 70.29%   Chen and Huang (2021) IEMOCAP, Aibo   Neutral, Happy, Sad, Angry. Angry, Emphatic, Neutral, Positive, Rest 3D spectrograms   2D CNN   73.1%, 41.1%   Zhao et al. (2021b) Emo-DB   Angry, Disgust, Fear, Happy, Natural   Log-mel spectrograms   Siamese Neural Network (SNN) 84.3%   Ntalampiras (2021) IEMOCAP, EMO-DB   Neutral, Happy, Sad, Angry. Angry, Disgust, Fear, Happy, Natural LMFCC, spectral features, Chroma, ZCR, pitch, Energy BLSTM-Directional Self-Attention 62.16%, 85.95%   Li et al. (2021a) IEMOCAP, RAVDESS, EMO-DB Neutral, Happy, Sad, Angry. Anger, Sad, Calm, Happy, Neutral, Fear, Disgust, Surprise. Anger, Sad, Happy, Neutral, Disgust, Fear, Bored Spectrogram   CNN   78.0%, 80.00%, 93.00% Kwon et al. (2021) NNIME   Angry, Sad, Happy, Frustration, Neutral, Surprise. OpenSMILE toolkit   BiLSTM with attention 61.92%   Hsu et al. (2021) IEMOCAP, RAVDESS   Neutral, Happy, Sad, Angry. Anger, Sad, Calm, Happy, Neutral, Fear, Disgust, Surprise. Magnitude, Spectrograms eGeMAPS, W2V2 features LSTM +   Wav2vec 2.0 66.3%, 77.5%   Pepino et al. (2021) LSSED   Angry, Natural, Happy, Sad.   Power spectrum   PyResNet   62.4%   Fan et al. (2021) AFEW5.0, BAUM-1   Anger, Joy, Sad, Disgust, Surprise, Fear, Neutral. Anger, Joy, Sad, Disgust, Surprise, Fear Automatic learning (1D raw waveform 2D Mel-spectrogram, 3D Temporal-spatial dynamic modeling) 1D CNN + 2D CNN +  3D CNN 35.77%, 44.06%   Zhang et al. (2021) IEMOCAP   Happy, Neutral, Angry, Sad.   log-Mel spectrogram with deltas and delta-deltas TCN + self-attention   66.10%   Zhao et al. (2021a) IEMOCAP, EMO-DB, ENTERFACE05, SAVEE Happy, Neutral, Angry, Sad.   Spectrogram   Attention   +   CNN   80.47%, 83.30%, 75.80%, 56.50% Li et al. (2021b) ( continued on next page )  segments and the other to sound type sound-labeled non-verbal audio segments. The feature extracted were embeddings and used as input for the final model, where they combine BiLSTM with attention and LSTM to build a sequence-to-sequence model. This study improved the accuracy compared to Huang et al. (2019); they used spectral features instead of waveform and used a deeper neural network, ResNet. These experiment results proved that nonverbal speech intervals improved emotion recognition performance. Therefore, instead of only verbal features, non-verbal vocalizations features could also be helpful.  9. Speech emotion recognition in the natural environment  These model approaches examined can be performed with all kinds of data covered in Section 4. However, utilized natural data contains

Page 20:
Speech Communication 154 (2023) 102974  20 A. Hashem et al.  Table 11   ( continued ). Database   Emotions recognized   Features used   Classifiers   Accuracy   Ref. IEMOCAP   Neutral, Happy, Sad, Fear, Anger, Frustrated. Spectrogram   CNN + LSTM   61%   Li et al. (2021c) ShEMO   Anger, Surprise, Happy, Sad, Neutral.   emo_large feature set   CNN   65.20%   Yazdani et al. (2021) 2020 RAVDESS, EMO-DB, IEMOCAP Calm, Angry, neutral, Fear, Happy, Sad, Disgust, Surprise. Neutral, Sad, Happy, Anger. Happy, Angry, Sad, Neutral. MFCC, Mel-scaled spectrogram, Chromagram, Spectral contrast feature, Tonnetz representation. CNN   71.61%, 86.1%, 95.71% Issa et al. (2020) ASVP-ESD   Happy, Angry, Sad, Neutral, Surprise, Fear.   Mel-scale spectrogram, MFCC, log-mel spectrogram, zero-crossing rate, chroma CNN-BiLSTM   74.39%   Landry et al. (2020) IEMOCAP, RAVDESS   Happy, Angry, Sad, Neutral. Calm, Angry, neutral, Fear, Happy, Sad, Disgust, Surprise. Spectrogram   CNN   81.75%, 79.5%   Kwon (2019) IEMOCAP   Happy, Angry, Sad, Neutral.   LLD, HSF, Mel-spectrogram   CNN + RNN + DNN   58.3%   Yao et al. (2020) IEMOCA, Emo-DB, SAVEE, EMOVO Happy, Neutral, Angry, Sad.   3D log Mel-spectrogram   CNN + RNN   65.13%, 84.22%, 73.83%, 65.80% Zayene et al. (2020) SSPNet   Laughter, Filler.   MFCC   LSTM   89.15%   Joshi et al. (2020) RML, EMO-DB, IEMOCAP Anger, Disgust, Fear, Happy, Sad, Surprise. Anger, Fear, Joy, Sad, Disgust, Bored, Neutral. Happy, Sad, Angry, Neutral. Hand-crafted low-level descriptors DNN   73.15%, 85.73%, 69.37% Xia et al. (2020) EmoDB, IEMOCAP   Happy, Anger, Neutral, Sad.   ZCR, RMS energy, F0, HNR, MFCC DNN   83.73%, 62.47%   Tiwari et al. (2020) SAVEE   Angry, Happy, Neutral, Sad.   Spectrogram   DSCNN   87.8%   Wani et al. (2020) EmoDB, IEMOCAP   Happy, Anger, Neutral, Sad.   MFCC, ZCR, RMS.   WADAN + DNN   84.49%, 66.92%   Yi and Mak (2020) 2019 NNIME   Angry, Sad, Happy, Frustration, Neutral, Surprise. Automatically learning   LSTM-based sequence-to- sequence model 52.00%   Huang et al. (2019) EMO-DB, CASIA   Anger, Boredom, Fear, Happiness, Neutrality, Sadness. log-Mel-spectrograms with static delta, and delta-delta CNN   82.5%, 51.3%   Jiang et al. (2019) IEMOCAP   Angry, Happy, Sad, Neutral.   Spectrogram   CNN   66.86%   Dai et al. (2019) RAVDESS   Calm, Angry,neutral, Fearful, Happy, Sad, Disgust, Surprise. MFCC   DNN   68.5%   Lee et al. (2019) EmoDB, IEMOCAP   Anger, Bored, Disgust, Fear, Happy, Neutral, Sad. Angry, Excited, Frustrated, Happy, Neutral, Sad. Spectrogram   1D, 2D CNN-LSTM   92.9%, 89.16%   Zhao et al. (2019b) SAVEE   Anger, Disgust, Fear, Happy, Sad, Surprise. MFCC   CNN   83.61%   Qayyum et al. (2019) Emo-DB, CASIA   Angry, Fear, Happy, Neutral, Sad, Surprise. Automatically learning   DRNN   92.08%, 90.41%   Xia et al. (2019) RAVDESS   Calm, Happy, Sad, Angry, Fear, Surprise, Disgust. Spectrogram   CNN   95%   Tariq et al. (2019) IEMOCAP, FAU-AEC   Angry, Happy, Neutral, Sad. Anger, Emphatic, Neutral, Positive, Rest. Spectrogram   Attention-BLSTM- FCN 68.1%, 45.4%   Zhao et al. (2019a) 2018 EmoDB   Anger, Fear, Bored, Disgust, Happy, Neutral, Sad.   Spectrogram   +   heuristic features CNN + ELM   92.50%   Guo et al. (2018) EmoDB   Anger, Fear, Bored, Disgust, Happy, Neutral, Sad.   Spectrogram   CNN   81.5%   Stolar et al. (2018) 2017 IEMOCAP   Happy, Angry, Sad, Neutral.   Spectrogram   CNN   64.78%   Fayek et al. (2017) ( continued on next page )  some limitations that must be considered. Nevertheless, the challenges in natural data are considered a good destination for researchers. These data can represent the real-life environment and develop a model that considers this data, confirming that the model will not drop when applied to real-life applications. However, some issues appeared with utilizing deep learning in natural data. Fahad et al. (2021) covered SER in the natural environment in detail. Three deep-learning issues related to the natural environment will be covered and how they can be improved: the lack of natural data, data imbalance, and the different length utterances. The lack of large natural speech data is a problem for

Page 25:
Speech Communication 154 (2023) 102974  25 A. Hashem et al.  The effect of using vocal burst features and how they can improve emotion recognition is discussed. Some problems encountered when using natural datasets are briefly covered, and different metrics used for measuring the performance of SER systems are also covered. Findings of the review, limitations, and future direction have also been covered. Current research focuses on various issues, including some that have not yet been solved, such as features, classifiers, cross-lingual emotion recognition, and real-time emotion recognition, which could be a mat- ter of SER research’s forthcoming studies. The scope of our current paper does not involve conducting simulation. It only includes a review of the previous studies related to SER in different aspects. Therefore, We plan to incorporate simulations to extend our research further, which will be integral to our future work, allowing us to simulate various methods and datasets and perform comparative analyses.  Declaration of competing interest  The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.  Data availability  No data was used for the research described in the article.  References  Abbaschian, B.J., Sierra-Sosa, D., Elmaghraby, A., 2021. Deep learning techniques for speech emotion recognition, from databases to models. Sensors 21 (4), 1249. Abdelhamid, A.A., 2023. Speech emotions recognition for online education. Fusion: Pract. Appl. 10 (1). Abdelwahab, M., Busso, C., 2018. Domain adversarial for acoustic emotion recognition. IEEE/ACM Trans. Audio Speech Lang. Process. 26 (12), 2423–2435. Aftab, A., Morsali, A., Ghaemmaghami, S., Champagne, B., 2022. LIGHT-SERNET: A lightweight fully convolutional neural network for speech emotion recognition. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 6912–6916. Akçay, M.B., Oğuz, K., 2020. Speech emotion recognition: Emotional models, databases, features,   preprocessing   methods,   supporting   modalities,   and   classifiers.   Speech Commun. 116, 56–76. Alam, M.J., Attabi, Y., Dumouchel, P., Kenny, P., O’Shaughnessy, D.D., 2013. Amplitude modulation features for emotion recognition from speech. In: INTERSPEECH. pp. 2420–2424. Anagnostopoulos, C.-N., Iliou, T., Giannoukos, I., 2015. Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011. Artif. Intell. Rev. 43, 155–177. Ancilin, J., Milton, A., 2021. Improved speech emotion recognition with mel frequency magnitude coefficient. Appl. Acoust. 179, 108046. Aslan,   M.S.,   Hailat,   Z.,   Alafif,   T.K.,   Chen,   X.-W.,   2017.   Multi-channel   multi-model feature learning for face recognition. Pattern Recognit. Lett. 85, 79–83. Atila, O., Şengür, A., 2021. Attention guided 3D CNN-LSTM model for accurate speech based emotion recognition. Appl. Acoust. 182, 108260. Atmaja, B.T., Akagi, M., 2020. The effect of silence feature in dimensional speech emotion recognition. arXiv preprint arXiv:2003.01277. Atmaja, B.T., Sasou, A., Akagi, M., 2022. Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion. Speech Commun.. Auckenthaler,   R.,   Carey,   M.,   Lloyd-Thomas,   H.,   2000.   Score   normalization   for text-independent   speaker   verification   systems.   Digit.   Signal   Process.   10   (1–3), 42–54. Audibert, N., Aubergé, V., Rilliard, A., 2007. When is the emotional information? A gating experiment for gradient and contours cues. In: Proceedings of ICPhS XVI Meeting. Saarbrucken. pp. 6–10. Ayadi, M., Kamel, M., Karray, F., 2011. Survey on speech recognition: Resources, features and methods. Pattern Recognit. 44 (3), 572–587. Baevski, A., Zhou, Y., Mohamed, A., Auli, M., 2020. Wav2vec 2.0: A framework for self-supervised learning of speech representations. Adv. Neural Inf. Process. Syst. 33, 12449–12460. Bao, W., Li, Y., Gu, M., Yang, M., Li, H., Chao, L., Tao, J., 2014. Building a Chinese natural emotional audio-visual database. In: 2014 12th International Conference on Signal Processing (ICSP). IEEE, pp. 583–587. Batliner, A., Schuller, B., Seppi, D., Steidl, S., Devillers, L., Vidrascu, L., Vogt, T., Aharonson, V., Amir, N., 2011. The Automatic Recognition of Emotions in Speech. Springer. Beigi, H., 2011. Fundamentals of Speaker Recognition. Springer Science & Business Media. Bengio, Y., LeCun, Y., et al., 2007. Scaling learning algorithms towards AI. Large-Scale Kernel Mach. 34 (5), 1–41. Bertero, D., Fung, P., 2017. A first look into a convolutional neural network for speech emotion detection. In: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 5115–5119. Bie, F., Wang, D., Zheng, T.F., Tejedor, J., Chen, R., 2013. Emotional adaptive training for speaker verification. In: 2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference. IEEE, pp. 1–4. Boateng, G., Fleisch, E., Kowatsch, T., 2022. Emotion recognition among couples: A survey. arXiv preprint arXiv:2202.08430. Boigne, J., Liyanage, B., Östrem, T., 2020. Recognizing more emotions with less data using self-supervised transfer learning. arXiv preprint arXiv:2011.05585. Bongirwar, V.K., Potnurwar, A., 2022. Song recommendation using speech emotion recognition. Int. J. Health Sci. 6 (S1), 10428–10434. Brigham, E.O., Morrow, R., 1967. The fast Fourier transform. IEEE Spectr. 4 (12), 63–70. Buhl, F.W., 2023. EmoGator: A new open source vocal burst dataset with baseline machine learning classification methodologies. arXiv preprint arXiv:2301.00508. Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W.F., Weiss, B., et al., 2005. A database of German emotional speech. In: Interspeech, Vol. 5. pp. 1517–1520. Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J.N., Lee, S., Narayanan, S.S., 2008. IEMOCAP: Interactive emotional dyadic motion capture database. Lang. Resour. Eval. 42 (4), 335–359. Cao, H., Cooper, D.G., Keutmann, M.K., Gur, R.C., Nenkova, A., Verma, R., 2014. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE Trans. Affect. Comput. 5 (4), 377–390. Chalapathi,   M.,   Kumar,   M.R.,   Sharma,   N.,   Shitharth,   S.,   2022.   Ensemble   learning by high-dimensional acoustic features for emotion recognition from speech audio signal. Secur. Commun. Netw. 2022. Chang,   J.,   Zhang,   X.,   Zhang,   Q.,   Sun,   Y.,   2018.   Investigating   duration   effects   of emotional speech stimuli in a tonal language by using event-related potentials. IEEE Access 6, 13541–13554. Chatziagapi,   A.,   Paraskevopoulos,   G.,   Sgouropoulos,   D.,   Pantazopoulos,   G.,   Nikan- drou, M., Giannakopoulos, T., Katsamanis, A., Potamianos, A., Narayanan, S., 2019. Data augmentation using GANs for speech emotion recognition. In: Interspeech. pp. 171–175. Chaudhari, P.R., Alex, J.S.R., 2016. Selection of features for emotion recognition from speech. Indian J. Sci. Technol. 9 (39), 1–5. Chavhan, Y., Dhore, M., Yesaware, P., 2010. Speech emotion recognition using support vector machine. Int. J. Comput. Appl. 1 (20), 6–9. Chen, Q., Huang, G., 2021. A novel dual attention-based BLSTM with hybrid features in speech emotion recognition. Eng. Appl. Artif. Intell. 102, 104277. Chen, L., Su, W., Feng, Y., Wu, M., She, J., Hirota, K., 2020. Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction. Inform. Sci. 509, 150–163. Copeland, M.E., 2002. Wellness recovery action plan: A system for monitoring, reducing and eliminating uncomfortable or dangerous physical symptoms and emotional feelings. Occup. Therapy Ment. Health 17 (3–4), 127–150. Cornew, L., Carver, L., Love, T., 2010. There’s more to emotion than meets the eye: A processing bias for neutral content in the domain of emotional prosody. Cognit. Emot. 24 (7), 1133–1152. Costantini, G., Iaderola, I., Paoloni, A., Todisco, M., 2014. EMOVO corpus: an Italian emotional speech database. In: International Conference on Language Resources and Evaluation (LREC 2014). European Language Resources Association (ELRA), pp. 3501–3504. Cowen, A.S., Laukka, P., Elfenbein, H.A., Liu, R., Keltner, D., 2019. The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures. Nat. Hum. Behav. 3 (4), 369–382. Cowie,   R.,   Douglas-Cowie,   E.,   Tsapatsoulis,   N.,   Votsis,   G.,   Kollias,   S.,   Fellenz,   W., Taylor, J.G., 2001. Emotion recognition in human-computer interaction. IEEE Signal Process. Mag. 18 (1), 32–80. Dai,   D.,   Wu,   Z.,   Li,   R.,   Wu,   X.,   Jia,   J.,   Meng,   H.,   2019.   Learning   discriminative features from spectrograms using center loss for speech emotion recognition. In: ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 7405–7409. Daneshfar, F., Kabudian, S.J., Neekabadi, A., 2020. Speech emotion recognition using hybrid spectral-prosodic features of speech signal/glottal waveform, metaheuristic- based   dimensionality   reduction,   and   Gaussian   elliptical   basis   function   network classifier. Appl. Acoust. 166, 107360. Deng,   J.,   Frühholz,   S.,   Zhang,   Z.,   Schuller,   B.,   2017a.   Recognizing   emotions   from whispered   speech   based   on   acoustic   feature   transfer   learning.   IEEE   Access   5, 5235–5246. Deng,   J.,   Xu,   X.,   Zhang,   Z.,   Frühholz,   S.,   Schuller,   B.,   2017b.   Semisupervised   au- toencoders for speech emotion recognition. IEEE/ACM Trans. Audio Speech Lang. Process. 26 (1), 31–43. Deng, J., Zhang, Z., Marchi, E., Schuller, B., 2013. Sparse autoencoder-based feature transfer learning for speech emotion recognition. In: 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. IEEE, pp. 511–516. Ekman, P., Friesen, W.V., Ellsworth, P., 2013. Emotion in the Human Face: Guidelines for Research and an Integration of Findings, Vol. 11. Elsevier.

Page 29:
Speech Communication 154 (2023) 102974  29 A. Hashem et al.  Ververidis, D., Kotropoulos, C., 2006. Emotional speech recognition: Resources, features, and methods. Speech Commun. 48 (9), 1162–1181. Vryzas, N., Kotsakis, R., Liatsou, A., Dimoulas, C.A., Kalliris, G., 2018. Speech emotion recognition for performance interaction. J. Audio Eng. Soc. 66 (6), 457–467. Wahlster, W., 2013. Verbmobil: Foundations of Speech-to-Speech Translation. Springer Science & Business Media. Wang, J., Han, Z., 2019. Research on speech emotion recognition technology based on deep and shallow neural network. In: 2019 Chinese Control Conference (CCC). IEEE, pp. 3555–3558. Wang, C., Ren, Y., Zhang, N., Cui, F., Luo, S., 2022. Speech emotion recognition based on multi-feature and multi-lingual fusion. Multimedia Tools Appl. 81 (4), 4897–4907. Wang,   K.,   Su,   G.,   Liu,   L.,   Wang,   S.,   2020.   Wavelet   packet   analysis   for speaker-independent emotion recognition. Neurocomputing 398, 257–264. Wang,   Z.-Q.,   Tashev,   I.,   2017.   Learning   utterance-level   representations   for   speech emotion and age/gender recognition using deep neural networks. In: 2017 IEEE International   Conference   on   Acoustics,   Speech   and   Signal   Processing   (ICASSP). IEEE, pp. 5150–5154. Wani,   T.M.,   Gunawan,   T.S.,   Qadri,   S.A.A.,   Kartiwi,   M.,   Ambikairajah,   E.,   2021.   A comprehensive   review   of   speech   emotion   recognition   systems.   IEEE   Access   9, 47795–47814. Wani, T.M., Gunawan, T.S., Qadri, S.A.A., Mansor, H., Kartiwi, M., Ismail, N., 2020. Speech emotion recognition using convolution neural networks and deep stride convolutional neural networks. In: 2020 6th International Conference on Wireless and Telematics (ICWT). IEEE, pp. 1–6. Weninger, F., Ringeval, F., Marchi, E., Schuller, B.W., 2016. Discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio. In: IJCAI, Vol. 2016. pp. 2196–2202. Williams, C.E., Stevens, K.N., 1972. Emotions and speech: Some acoustical correlates. J. Acoust. Soc. Amer. 52 (4B), 1238–1250. Wongpatikaseree, K., Singkul, S., Hnoohom, N., Yuenyong, S., 2022. Real-time end-to- end speech emotion recognition with cross-domain adaptation. Big Data Cognit. Comput. 6 (3), 79. Xia,   X.,   Jiang,   D.,   Sahli,   H.,   2020.   Learning   salient   segments   for   speech   emotion recognition using attentive temporal pooling. IEEE Access 8, 151740–151752. Xia, G., Li, F., Zhao, D., Zhang, Q., Yang, S., 2019. Fi-net: a speech emotion recognition framework with feature integration and data augmentation. In: 2019 5th Interna- tional Conference on Big Data Computing and Communications (BIGCOM). IEEE, pp. 195–203. Xu, G., Li, W., Liu, J., 2020. A social emotion classification approach using multi-model fusion. Future Gener. Comput. Syst. 102, 347–356. Xu, X., Li, D., Zhou, Y., Wang, Z., 2022. Multi-type features separating fusion learning for speech emotion recognition. Appl. Soft Comput. 130, 109648. Yadav, S.P., Zaidi, S., Mishra, A., Yadav, V., 2022. Survey on machine learning in speech emotion recognition and vision systems using a recurrent neural network (RNN). Arch. Comput. Methods Eng. 29 (3), 1753–1770. Yang, Y., Peng, Z., Zhang, W., Meng, G., 2019. Parameterised time-frequency analysis methods and their engineering applications: A review of recent advances. Mech. Syst. Signal Process. 119, 182–221. Yang, N., Yuan, J., Zhou, Y., Demirkol, I., Duan, Z., Heinzelman, W., Sturge-Apple, M., 2017. Enhanced multiclass SVM with thresholding fusion for speech-based emotion classification. Int. J. Speech Technol. 20, 27–41. Yao,   Z.,   Wang,   Z.,   Liu,   W.,   Liu,   Y.,   Pan,   J.,   2020.   Speech   emotion   recognition using fusion of three multi-task learning-based classifiers: HSF-DNN, MS-CNN and LLD-RNN. Speech Commun. 120, 11–19. Yazdani, A., Simchi, H., Shekofteh, Y., 2021. Emotion recognition in persian speech using deep neural networks. In: 2021 11th International Conference on Computer Engineering and Knowledge (ICCKE). IEEE, pp. 374–378. Ye, J., Wen, X., Wei, Y., Xu, Y., Liu, K., Shan, H., 2022. Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition. arXiv preprint arXiv:2211.08233. Yi, L., Mak, M.-W., 2020. Improving speech emotion recognition with adversarial data augmentation network. IEEE Trans. Neural Netw. Learn. Syst. 33 (1), 172–184. Yildirim,   S.,   Kaya,   Y.,   Kılıç,   F.,   2021.   A   modified   feature   selection   method   based on metaheuristic algorithms for speech emotion recognition. Appl. Acoust. 173, 107721. Yiming,   S.,   Rui,   W.,   2015.   Voice   activity   detection   based   on   the   improved   dual- threshold method. In: 2015 International Conference on Intelligent Transportation, Big Data and Smart City. IEEE, pp. 996–999. Zão, L., Cavalcante, D., Coelho, R., 2014. Time-frequency feature and AMS-GMM mask for acoustic emotion classification. IEEE Signal Process. Lett. 21 (5), 620–624. Zayene, B., Jlassi, C., Arous, N., 2020. 3D convolutional recurrent global neural network for speech emotion recognition. In: 2020 5th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP). IEEE, pp. 1–5. Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S., 2007. A survey of affect recognition methods: audio, visual and spontaneous expressions. In: Proceedings of the 9th International Conference on Multimodal Interfaces. pp. 126–133. Zhang, S., Tao, X., Chuang, Y., Zhao, X., 2021. Learning deep multimodal affective features for spontaneous speech emotion recognition. Speech Commun. 127, 73–81. Zhang, S., Zhang, S., Huang, T., Gao, W., 2017. Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching. IEEE Trans. Multimed. 20 (6), 1576–1590. Zhao, Z., Bao, Z., Zhang, Z., Cummins, N., Sun, S., Wang, H., Tao, J., Schuller, B.W., 2021a. Self-attention transfer networks for speech emotion recognition. Virtual Real. Intell. Hardw. 3 (1), 43–54. Zhao, Z., Bao, Z., Zhao, Y., Zhang, Z., Cummins, N., Ren, Z., Schuller, B., 2019a. Explor- ing deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition. IEEE Access 7, 97515–97525. Zhao, Z., Li, Q., Zhang, Z., Cummins, N., Wang, H., Tao, J., Schuller, B.W., 2021b. Combining a parallel 2D CNN with a self-attention dilated residual network for CTC-based discrete speech emotion recognition. Neural Netw. 141, 52–60. Zhao, J., Mao, X., Chen, L., 2019b. Speech emotion recognition using deep 1D & 2D CNN LSTM networks. Biomed. Signal Process. Control 47, 312–323. Zheng, W., Yu, J., Zou, Y., 2015. An experimental study of speech emotion recognition based on deep convolutional neural networks. In: 2015 International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, pp. 827–831. Zhou, X., Guo, J., Bie, R., 2016. Deep learning based affective model for speech emotion recognition. In: 2016 Intl IEEE Conferences on Ubiquitous Intelligence & Comput- ing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld). IEEE, pp. 841–846. Zhou, G., Hansen, J.H., Kaiser, J.F., 2001. Nonlinear feature based classification of speech under stress. IEEE Trans. Speech Audio Process. 9 (3), 201–216. Zou, H., Si, Y., Chen, C., Rajan, D., Chng, E.S., 2022. Speech emotion recognition with co-attention based multi-level acoustic information. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 7367–7371.

Page 27:
Speech Communication 154 (2023) 102974  27 A. Hashem et al.  Kurpukdee, N., Koriyama, T., Kobayashi, T., Kasuriya, S., Wutiwiwatchai, C., Lam- srichan, P., 2017. Speech emotion recognition using convolutional long short-term memory neural network and support vector machines. In: 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, pp. 1744–1749. Kwon, S., 2019. A CNN-assisted enhanced audio signal processing for speech emotion recognition. Sensors 20 (1), 183. Kwon, S., et al., 2021. Att-net: Enhanced emotion recognition system using lightweight self-attention module. Appl. Soft Comput. 102, 107101. Lacheret, A., Grichkovtsova, I., Morel, M., Beaucousin, V., Tzourio-Mazoyer, N., 2007. Affective speech gating. In: 16th International Congress of Phonetic Sciences (ICPhS XVI), ID 1539. Saarbrucken Univ. des Saarlandes, pp. 805–808. Lal, T.N., Chapelle, O., Weston, J., Elisseeff, A., 2006. Embedded methods. Feature Extr.: Found. Appl. 137–165. Landry, D.T.T., He, Q., Yan, H., Li, Y., 2020. ASVP-ESD: A dataset and its benchmark for emotion recognition using both speech and non-speech utterances. Glob. Sci. J. (ISSN: 2320-9186) 8 (5), 4237–4249. Langari, S., Marvi, H., Zahedi, M., 2020. Efficient speech emotion recognition using modified feature extraction. Inform. Med. Unlocked 20, 100424. Latif,   S.,   Qayyum,   A.,   Usman,   M.,   Qadir,   J.,   2018.   Cross   lingual   speech   emotion recognition: Urdu vs. western languages. In: 2018 International Conference on Frontiers of Information Technology (FIT). IEEE, pp. 88–93. Latif, S., Rana, R., Khalifa, S., Jurdak, R., Qadir, J., Schuller, B.W., 2021. Survey of deep representation learning for speech emotion recognition. IEEE Trans. Affect. Comput.. Lee, K.H., Choi, H.K., Jang, B.T., et al., 2019. A study on speech emotion recognition using a deep neural network. In: 2019 International Conference on Information and Communication Technology Convergence (ICTC). IEEE, pp. 1162–1165. Lee, C.M., Narayanan, S.S., 2005. Toward detecting emotions in spoken dialogs. IEEE Trans. Speech Audio Process. 13 (2), 293–303. Lee, C.M., Narayanan, S.S., Pieraccini, R., 2002. Classifying emotions in human-machine spoken dialogs. In: Proceedings. IEEE International Conference on Multimedia and Expo, Vol. 1. IEEE, pp. 737–740. Li,   X.,   Akagi,   M.,   2019.   Improving   multilingual   speech   emotion   recognition   by combining acoustic features in a three-layer model. Speech Commun. 110, 1–12. Li, Y., Chao, L., Liu, Y., Bao, W., Tao, J., 2015. From simulated speech to natural speech, what are the robust features for emotion recognition? In: 2015 International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, pp. 368–373. Li, D., Liu, J., Yang, Z., Sun, L., Wang, Z., 2021a. Speech emotion recognition using recurrent neural networks with directional self-attention. Expert Syst. Appl. 173, 114683. Li, X., Song, W., Liang, Z., 2020. Emotion recognition from speech using deep learning on spectrograms. J. Intell. Fuzzy Systems 39 (3), 2791–2796. Li, Y., Tao, J., Chao, L., Bao, W., Liu, Y., 2017. CHEAVD: a Chinese natural emotional audio–visual database. J. Ambient Intell. Humaniz. Comput. 8 (6), 913–924. Li, S., Xing, X., Fan, W., Cai, B., Fordson, P., Xu, X., 2021b. Spatiotemporal and frequen- tial cascaded attention networks for speech emotion recognition. Neurocomputing 448, 238–248. Li, H., Zhang, X., Wang, M.-J., 2021c. Research on speech emotion recognition based on deep neural network. In: 2021 IEEE 6th International Conference on Signal and Image Processing (ICSIP). IEEE, pp. 795–799. Li, L., Zhao, Y., Jiang, D., Zhang, Y., Wang, F., Gonzalez, I., Valentin, E., Sahli, H., 2013. Hybrid deep neural network–hidden markov model (dnn-hmm) based speech emo- tion recognition. In: 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction. IEEE, pp. 312–317. Lian, Z., Tao, J., Liu, B., Huang, J., Yang, Z., Li, R., 2020. Context-dependent domain adversarial neural network for multimodal emotion recognition. In: INTERSPEECH. pp. 394–398. Lieskovská, E., Jakubec, M., Jarina, R., Chmulík, M., 2021. A review on speech emotion recognition using deep learning and attention mechanism. Electronics 10 (10), 1163. Lim, W., Jang, D., Lee, T., 2016. Speech emotion recognition using convolutional and recurrent neural networks. In: 2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). IEEE, pp. 1–4. Lin, Y.-L., Wei, G., 2005. Speech emotion recognition based on HMM and SVM. In: 2005 International Conference on Machine Learning and Cybernetics, Vol. 8. IEEE, pp. 4898–4901. Liu, Z.-T., Rehman, A., Wu, M., Cao, W.-H., Hao, M., 2021. Speech emotion recognition based on formant characteristics feature extraction and phoneme type convergence. Inform. Sci. 563, 309–325. Liu, J., Wang, H., Sun, M., Wei, Y., 2022. Graph based emotion recognition with attention pooling for variable-length utterances. Neurocomputing 496, 46–55. Liu, Z.-T., Wu, M., Cao, W.-H., Mao, J.-W., Xu, J.-P., Tan, G.-Z., 2018. Speech emotion recognition based on feature selection and extreme learning machine decision tree. Neurocomputing 273, 271–280. Livingstone, S.R., Russo, F.A., 2018. The ryerson audio-visual database of emotional speech   and   song   (RAVDESS):   A   dynamic,   multimodal   set   of   facial   and   vocal expressions in north American english. PLoS One 13 (5), e0196391. Lokhande, N.N., Nehe, N.S., Vikhe, P.S., 2012. Voice activity detection algorithm for speech recognition applications. In: IJCA Proceedings on International Conference in Computational Intelligence (ICCIA2012), Vol. Iccia, Vol. 6. pp. 1–4. Lotfian, R., Busso, C., 2017. Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings. IEEE Trans. Affect. Comput. 10 (4), 471–483. Luo, D., Zou, Y., Huang, D., 2017. Speech emotion recognition via ensembling neural networks.   In:   2017   Asia-Pacific   Signal   and   Information   Processing   Association Annual Summit and Conference (APSIPA ASC). IEEE, pp. 1351–1355. Madanian, S., Parry, D., Adeleye, O., Poellabauer, C., Mirza, F., Mathew, S., Schnei- der,   S.,   2022.   Automatic   speech   emotion   recognition   using   machine   learning: Digital transformation of mental health. Mairano, P., Zovato, E., Quinci, V., 2019. Do sentiment analysis scores correlate with acoustic features of emotional speech? In: AISV Conf.. Maldonado, S., Weber, R., 2009. A wrapper method for feature selection using support vector machines. Inform. Sci. 179 (13), 2208–2217. Manohar, K., Logashanmugam, E., 2022. Hybrid deep learning with optimal feature selection for speech emotion recognition using improved meta-heuristic algorithm. Knowl.-Based Syst. 246, 108659. Mansouri, B.Z., Ghaffary, H.R., Harimi, A., 2022. Speech emotion recognition using sub-band   spectrogram   fusion   and   deep   convolutional   neural   network   transfer learning. Mao, Q., Xu, G., Xue, W., Gou, J., Zhan, Y., 2017. Learning emotion-discriminative and domain-invariant features for domain adaptation in speech emotion recognition. Speech Commun. 93, 1–10. Martin, O., Kotsia, I., Macq, B., Pitas, I., 2006. The enterface’05 audio-visual emotion database.   In:   22nd   International   Conference   on   Data   Engineering   Workshops (ICDEW’06). IEEE, p. 8. McKeown, G., Valstar, M., Cowie, R., Pantic, M., Schroder, M., 2011. The semaine database:   Annotated   multimodal   records   of   emotionally   colored   conversations between a person and a limited agent. IEEE Trans. Affect. Comput. 3 (1), 5–17. Mehrabian, A., Russell, J.A., 1974. An Approach to Environmental Psychology. the MIT Press. Mencattini,   A.,   Martinelli,   E.,   Costantini,   G.,   Todisco,   M.,   Basile,   B.,   Bozzali,   M., Di   Natale,   C.,   2014.   Speech   emotion   recognition   using   amplitude   modulation parameters and a combined feature selection procedure. Knowl.-Based Syst. 63, 68–81. Minardi, H., 2013. Emotion recognition by mental health professionals and students. Nurs. Stand. 27 (25). Mirsamadi, S., Barsoum, E., Zhang, C., 2017. Automatic speech emotion recognition using   recurrent   neural   networks   with   local   attention.   In:   2017   IEEE   Interna- tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 2227–2231. Mustafa,   M.B.,   Yusoof,   M.A.,   Don,   Z.M.,   Malekzadeh,   M.,   2018.   Speech   emotion recognition research: an analysis of research focus. Int. J. Speech Technol. 21 (1), 137–156. Nema,   B.M.,   Abdul-Kareem,   A.A.,   2018.   Preprocessing   signal   for   speech   emotion recognition. Al-Mustansiriyah J. Sci. 28 (3), 157–165. Nordström, H., Laukka, P., 2019. The time course of emotion recognition in speech and music. J. Acoust. Soc. Am. 145 (5), 3058–3074. Ntalampiras,   S.,   2021.   Speech   emotion   recognition   via   learning   analogies.   Pattern Recognit. Lett. 144, 21–26. Ntalampiras,   S.,   Fakotakis,   N.,   2011.   Modeling   the   temporal   evolution   of   acoustic parameters for speech emotion recognition. IEEE Trans. Affect. Comput. 3 (1), 116–125. Nwe, T.L., Foo, S.W., De Silva, L.C., 2003. Speech emotion recognition using hidden Markov models. Speech Commun. 41 (4), 603–623. Nweke, H.F., Teh, Y.W., Al-Garadi, M.A., Alo, U.R., 2018. Deep learning algorithms for human activity recognition using mobile and wearable sensor networks: State of the art and research challenges. Expert Syst. Appl. 105, 233–261. Ozseven, T., 2018. Evaluation of the effect of frame size on speech emotion recognition. In: 2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT). IEEE, pp. 1–4. Özseven, T., 2019. A novel feature selection method for speech emotion recognition. Appl. Acoust. 146, 320–326. Paliwal, K.K., Lyons, J.G., Wójcicki, K.K., 2010. Preference for 20–40 ms window duration   in   speech   analysis.   In:   2010   4th   International   Conference   on   Signal Processing and Communication Systems. IEEE, pp. 1–4. Pandey, S.K., Shekhawat, H.S., Prasanna, S.M., 2019. Deep learning techniques for speech   emotion   recognition:   A   review.   In:   2019   29th   International   Conference Radioelektronika (RADIOELEKTRONIKA). IEEE, pp. 1–6. Papakostas,   M.,   Spyrou,   E.,   Giannakopoulos,   T.,   Siantikos,   G.,   Sgouropoulos,   D., Mylonas, P., Makedon, F., 2017. Deep visual attributes vs. hand-crafted audio features on multidomain speech emotion recognition. Computation 5 (2), 26. Pell, M.D., Kotz, S.A., 2011. On the time course of vocal emotion recognition. PLoS One 6 (11), e27256. Pepino, L., Riera, P., Ferrer, L., 2021. Emotion recognition from speech using wav2vec 2.0 embeddings. arXiv preprint arXiv:2104.03502. Pichora-Fuller, M.K., Dupuis, K., 2020. Toronto emotional speech set (TESS). Scholars Port. Dataverse 1, 2020.

Page 28:
Speech Communication 154 (2023) 102974  28 A. Hashem et al.  Plutchik, R., 2001. The nature of emotions: Human emotions have deep evolutionary roots,   a   fact   that   may   explain   their   complexity   and   provide   tools   for   clinical practice. Amer. Sci. 89 (4), 344–350. Pohjalainen, J., Fabien Ringeval, F., Zhang, Z., Schuller, B., 2016. Spectral and cepstral audio noise reduction techniques in speech emotion recognition. In: Proceedings of the 24th ACM International Conference on Multimedia. pp. 670–674. Pollack, I., Rubenstein, H., Horowitz, A., 1960. Communication of verbal modes of expression. Lang. Speech 3 (3), 121–130. Popović, B., Stanković, I., Ostrogonac, S., 2013. Temporal discrete cosine transform for speech emotion recognition. In: 2013 IEEE 4th International Conference on Cognitive Infocommunications (CogInfoCom). IEEE, pp. 87–90. Prabhu, K.M., 2014. Window Functions and their Applications in Signal Processing. Přibil, J., Přibilová, A., 2013. Evaluation of influence of spectral and prosodic features on GMM classification of Czech and Slovak emotional speech. EURASIP J. Audio Speech Music Process. 2013, 1–22. Puterka, B., Kacur, J., 2018. Time window analysis for automatic speech emotion recognition. In: 2018 International Symposium ELMAR. IEEE, pp. 143–146. Puterka,   B.,   Kacur,   J.,   Pavlovicova,   J.,   2019.   Windowing   for   speech   emotion recognition. In: 2019 International Symposium ELMAR. IEEE, pp. 147–150. Qayyum, A.B.A., Arefeen, A., Shahnaz, C., 2019. Convolutional neural network (CNN) based   speech-emotion   recognition.   In:   2019   IEEE   International   Conference   on Signal Processing, Information, Communication & Systems (SPICSCON). IEEE, pp. 122–125. Raj, A.A.E., Shajivan, S., Rohit, A., et al., 2023. Speech emotion recognition using deep learning. In: 2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA). IEEE, pp. 505–509. Rambabu, B., Botsa, K.K., Paidi, G., Gangashetty, S.V., 2020. IIIT-H TEMD semi-natural emotional speech database from professional actors and non-actors. In: Proceedings of the 12th Language Resources and Evaluation Conference. pp. 1538–1545. Rao, K.S., Koolagudi, S.G., Vempada, R.R., 2013. Emotion recognition from speech using global and local prosodic features. Int. J. Speech Technol. 16, 143–160. Requardt, A.F., Ihme, K., Wilbrink, M., Wendemuth, A., 2020. Towards affect-aware vehicles for increasing safety and comfort: recognising driver emotions from audio recordings in a realistic driving study. IET Intell. Transp. Syst. 14 (10), 1265–1277. Rigoulot, S., Wassiliwizky, E., Pell, M.D., 2013. Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition. Front. Psychol. 4, 367. Ringeval, F., Sonderegger, A., Sauer, J., Lalanne, D., 2013. Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions. In: 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG). IEEE, pp. 1–8. Rousseau, A., Deléglise, P., Esteve, Y., et al., 2014. Enhancing the TED-LIUM corpus with   selected   data   for   language   modeling   and   more   TED   talks.   In:   LREC.   pp. 3935–3939. Rumelhart,   D.E.,   Hinton,   G.E.,   Williams,   R.J.,   1986.   Learning   representations   by back-propagating errors. Nature 323 (6088), 533–536. Russell, J.A., Mehrabian, A., 1977. Evidence for a three-factor theory of emotions. J. Res. Pers. 11 (3), 273–294. Saad, F., Mahmud, H., Shaheen, M., Hasan, M., Farastu, P., et al., 2021. Is speech emo- tion recognition language-independent? Analysis of english and bangla languages using language-independent vocal features. arXiv preprint arXiv:2111.10776. Scherer, K.R., 1995. Expression of emotion in voice and music. J. Voice 9 (3), 235–248. Scherer, K.R., 2005. What are emotions? And how can they be measured? Soc. Sci. Inf. 44 (4), 695–729. Schiel, F., Steininger, S., Türk, U., 2002. The SmartKom multimodal corpus at BAS. In: LREC. Citeseer. Schroder, M., Cowie, R., 2006. Issues in emotion-oriented computing-towards a shared understanding. In: Workshop on Emotion and Computing. Citeseer. Schuller, B., 2011. Recognizing affect from linguistic information in 3D continuous space. IEEE Trans. Affect. Comput. 2 (4), 192–205. Schuller,   B.W.,   2018.   Speech   emotion   recognition:   Two   decades   in   a   nutshell, benchmarks, and ongoing trends. Commun. ACM 61 (5), 90–99. Schuller, B.W., Batliner, A., Bergler, C., Messner, E.-M., Hamilton, A., Amiriparian, S., Baird, A., Rizos, G., Schmitt, M., Stappen, L., et al., 2020. The interspeech 2020 computational paralinguistics challenge: Elderly emotion, breathing & masks. Schuller, B., Batliner, A., Steidl, S., Seppi, D., 2011. Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge. Speech Commun. 53 (9–10), 1062–1087. Schuller, B., Rigoll, G., Lang, M., 2003. Hidden Markov model-based speech emotion recognition. In: 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03), Vol. 2. IEEE, pp. II–1. Seehapoch, T., Wongthanavasu, S., 2013. Speech emotion recognition using support vector machines. In: 2013 5th International Conference on Knowledge and Smart Technology (KST). IEEE, pp. 86–91. Sefara, T.J., Mokgonyane, T.B., 2020. Emotional speaker recognition based on ma- chine and deep learning. In: 2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC). IEEE, pp. 1–8. Shah,   F.,   et   al.,   2017.   Wavelet   packets   for   speech   emotion   recognition.   In:   2017 Third International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB). IEEE, pp. 479–481. Shaukat,   A.,   Chen,   K.,   2010.   Exploring   language-independent   emotional   acoustic features via feature selection. arXiv preprint arXiv:1009.0117. Sheikhan, M., Bejani, M., Gharavian, D., 2013. Modular neural-SVM scheme for speech emotion recognition using ANOVA feature selection method. Neural Comput. Appl. 23, 215–227. Shih, P.-Y., Chen, C.-P., Wang, H.-M., 2017. Speech emotion recognition with skew- robust   neural   networks.   In:   2017   IEEE   International   Conference   on   Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 2751–2755. Siadat, S.R., Voronkov, I.M., Kharlamov, A.A., 2022. Emotion recognition from Per- sian speech with 1D convolution neural network. In: 2022 Fourth International Conference Neurotechnologies and Neurointerfaces (CNN). IEEE, pp. 152–157. Siami-Namini, S., Tavakoli, N., Namin, A.S., 2019. The performance of LSTM and BiLSTM in forecasting time series. In: 2019 IEEE International Conference on Big Data (Big Data). IEEE, pp. 3285–3292. Siegert,   I.,   Böck,   R.,   Wendemuth,   A.,   2018.   Using   a   PCA-based   dataset   similarity measure to improve cross-corpus emotion recognition. Comput. Speech Lang. 51, 1–23. Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Singh, Y.B., Goel, S., 2022. A systematic literature review of speech emotion recognition approaches. Neurocomputing. Singkul, S., Woraratpanya, K., 2022. Vector learning representation for generalized speech emotion recognition. Heliyon 8 (3), e09196. Sinith, M., Aswathi, E., Deepa, T., Shameema, C., Rajan, S., 2015. Emotion recognition from audio signals using support vector machine. In: 2015 IEEE Recent Advances in Intelligent Computational Systems (RAICS). IEEE, pp. 139–144. Steidl, S., 2009. Automatic Classification of Emotion Related User States in Spontaneous Children’s Speech. Logos-Verlag Berlin, Germany. Stolar,   M.N.,   Lech,   M.,   Bolia,   R.S.,   Skinner,   M.,   2017.   Real   time   speech   emotion recognition using RGB image classification and transfer learning. In: 2017 11th In- ternational Conference on Signal Processing and Communication Systems (ICSPCS). IEEE, pp. 1–8. Stolar, M., Lech, M., Bolia, R.S., Skinner, M., 2018. Acoustic characteristics of emo- tional speech using spectrogram image classification. In: 2018 12th International Conference on Signal Processing and Communication Systems (ICSPCS). IEEE, pp. 1–5. Sugan, N., Srinivas, N.S., Kar, N., Kumar, L., Nath, M.K., Kanhe, A., 2018. Performance comparison of different cepstral features for speech emotion recognition. In: 2018 International CET Conference on Control, Communication, and Computing (IC4). IEEE, pp. 266–271. Sun, L., Fu, S., Wang, F., 2019a. Decision tree SVM model with Fisher feature selection for speech emotion recognition. EURASIP J. Audio Speech Music Process. 2019 (1), 1–14. Sun, Y., Wen, G., 2015. Emotion recognition using semi-supervised feature selection with speaker normalization. Int. J. Speech Technol. 18 (3), 317–331. Sun, L., Zou, B., Fu, S., Chen, J., Wang, F., 2019b. Speech emotion recognition based on DNN-decision tree SVM model. Speech Commun. 115, 29–37. Swain, M., Maji, B., Kabisatpathy, P., Routray, A., 2022. A DCRNN-based ensemble classifier for speech emotion recognition in odia language. Complex Intell. Syst. 8 (5), 4237–4249. Swain, M., Routray, A., Kabisatpathy, P., 2018. Databases, features and classifiers for speech emotion recognition: a review. Int. J. Speech Technol. 21 (1), 93–120. Tang,   D.,   Zeng,   J.,   Li,   M.,   2018.   An   end-to-end   deep   learning   framework   for speech emotion recognition of atypical individuals. In: Interspeech, Vol. 2018. pp. 162–166. Tariq, Z., Shah, S.K., Lee, Y., 2019. Speech emotion detection using iot based deep learning for health care. In: 2019 IEEE International Conference on Big Data (Big Data). IEEE, pp. 4191–4196. Tarunika, K., Pradeeba, R., Aruna, P., 2018. Applying machine learning techniques for speech emotion recognition. In: 2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT). IEEE, pp. 1–5. Tawari, A., Trivedi, M.M., 2010. Speech emotion analysis: Exploring the role of context. IEEE Trans. Multimed. 12 (6), 502–509. Teager, H., 1980. Some observations on oral air flow during phonation. IEEE Trans. Acoust. Speech Signal Process. 28 (5), 599–601. Teager, H.M., Teager, S.M., 1983. A phenomenological model for vowel production in the vocal tract. Speech Sci.: Recent Adv. 73–109. Teixeira, J.P., Oliveira, C., Lopes, C., 2013. Vocal acoustic analysis–jitter, shimmer and hnr parameters. Proc. Technol. 9, 1112–1122. Thirumuru,   R.,   Gurugubelli,   K.,   Vuppala,   A.K.,   2022.   Novel   feature   representation using single frequency filtering and nonlinear energy operator for speech emotion recognition. Digit. Signal Process. 120, 103293. Thu, L.N., Win, A., Oo, H.N., 2019. A review for reduction of noise by wavelet transform in audio signals. 06. Tiwari,   U.,   Soni,   M.,   Chakraborty,   R.,   Panda,   A.,   Kopparapu,   S.K.,   2020.   Multi- conditioning   and   data   augmentation   using   generative   noise   model   for   speech emotion   recognition   in   noisy   conditions.   In:   ICASSP   2020-2020   IEEE   Interna- tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 7194–7198. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Adv. Neural Inf. Process. Syst. 30.

Page 5:
Speech Communication 154 (2023) 102974  5 A. Hashem et al.  Table 3  Comparison of the proposed survey with the existing surveys in speech emotion recognition ( ✓ Include, x Not Include,   ∙   Partial). Ref.   Data   Emotion modeling Features   Preprocess   Feature selection Classifiers   Deep learning Transfer learning SER in Natural environment Vocal bursts Evaluation metrics SLR/LR Schuller et al. (2011)  ✓   ✓   ✓   ✓   ✓   ✓   x   x   ✓   x   ✓   LR Ayadi et al. (2011)  ✓   ✓   ✓   ✓   x   ✓   x   x   x   x   x   LR Koolagudi and Rao (2012)  ✓   x   ✓   x   x   ✓   x   x   x   x   x   LR Anagnostopoulos et al. (2015) x   ∙   ✓   x   ✓   x   x   x   x   ∙   x   LR Swain et al. (2018)  ✓   ✓   ✓   x   x   ✓   x   x   x   x   x   LR Mustafa et al. (2018)  ✓   ✓   ✓   x   x   ✓   x   x   x   x   x   SLR Schuller (2018)   ✓   ✓   ✓   x   x   ✓   ✓   ∙   x   x   ✓   LR Gunawan et al. (2018)  ✓   x   ✓   ✓   x   ✓   ∙   x   x   x   x   LR Khalil et al. (2019)  ✓   ✓   ✓   x   ✓   ✓   ✓   x   x   x   x   LR Akçay and Oğuz (2020)  ✓   ✓   ✓   ✓   ✓   ✓   ✓   ✓   x   x   x   LR Abbaschian et al. (2021)  ✓   x   ✓   x   x   ✓   ✓   ✓   x   x   x   LR Yadav et al. (2022)  ✓   ✓   ✓   ✓   x   ✓   ✓   ✓   x   x   x   LR Jahangir et al. (2021)  ✓   x   ✓   ✓   ✓   ✓   ✓   ✓   x   x   ✓   LR Latif et al. (2021)   ✓   ∙   ∙   x   x   ✓   ✓   ✓   x   x   ✓   LR Wani et al. (2021)   ✓   ∙   ✓   ✓   x   ✓   ✓   x   x   x   x   LR Lieskovská et al. (2021)  ✓   ✓   ✓   x   ✓   ✓   ✓   ✓   x   x   ✓   LR Fahad et al. (2021)  ✓   ✓   ✓   ✓   ✓   ✓   ✓   x   ✓   x   ✓   LR Atmaja et al. (2022)  ✓   ✓   ✓   x   x   ✓   ✓   x   𝑥   x   ✓   LR Singh and Goel (2022)  ✓   ∙   ✓   x   x   ✓   ✓   x   𝑥   x   x   SLR This survey   ✓   ✓   ✓   ✓   ✓   ✓   ✓   ✓   ✓   ✓   ✓   SLR  prosody, excitation, and vocal tract and covered combining features for emotion recognition. Deep learning, hybrid, and fusion techniques were also highlighted in their paper for emotion classification. Mustafa et al. (2018) examined articles published between 2006 and 2017. They covered and pointed out cross-lingual and real-time SER as the current focuses of SER. Schuller (2018) provided a brief survey covering traditional and current SER system techniques. They concentrated on the end-to-end SER system and covered the existing SER challenge from different languages and cultures. Gunawan et al. (2018) presented a comprehensive review of SER systems. They focused on feature extraction, classifier, and speech emotion database. However, all of them did not discuss the use of vocal burst features in SER. More recently, surveys aimed to cover the primary pipeline of SER and the deep learning models since most SOTA results are based on it. Khalil et al. (2019) covered more than just the essential aspects of SER. Moreover, they summarize DL approaches and discuss the current stud- ies using these techniques for SER. The survey discusses contributions made to speech emotion recognition and the limitations associated with it. However, they did not discuss the available approaches to overcoming weaknesses. Afterward, Akçay and Oğuz (2020) introduced a survey covering almost all SER aspects, including emotion models, databases, features, supporting modalities, and classifiers, and dis- cussed deep-learning classifiers and deep-learning-based enhancement techniques such as auto-encoder, multi-tasking, adversarial training, and attention in-depth. Also, they listed the current challenge in SER systems. However, they did not discuss the SER issues. After that, Abbaschian et al. (2021) focused on SER deep learning and unified experimental results that propose approaches to improve the results of existing methods. They also cover the available datasets and con- ventional machine-learning techniques for SER and give a comparison between practical neural network techniques in SER from different aspects. They covered deep-learning techniques but did not cover the SER in a natural environment. Yadav et al. (2022) conducted a survey focusing on speech emotion recognition and visual systems. They examine machine learning archi- tectures along with system and speech and vision processes. They dis- cussed databases, features, preprocessing methods, system modalities, classifiers, and emotional models and covered the issues for designing the emotional database. Jahangir et al. (2021) presented a compre- hensive review of existing deep learning techniques for SER, including their strengths and weaknesses. They discussed speech processing meth- ods, performance metrics, and publicly accessible emotional speech databases. Furthermore, they discuss the significance of the primary study findings and the issues and challenges that need to be addressed to enhance the field of SER systems. Latif et al. (2021) conducted a survey that reviewed deep learning approaches for speech-emotional representation learning. They covered databases, classifiers, and evalu- ation metrics. Wani et al. (2021) focused on giving a detailed analysis of how deep learning techniques can be utilized to learn speech-emotional representations. They discuss databases, speech processing, feature ex- traction techniques, and different classification methods used for SER. They pointed out different challenges faced in SER. Lieskovská et al. (2021) conducted a survey that focused on the most recent advance- ments in SER and discussed how different attentional strategies affected SER performance. They compared the system accuracies performed on a widely utilized IEMOCAP benchmark database. They discussed emotional speech database development, speech feature extraction, DL- based emotion modeling, and selected techniques for improving SER performance. All the above surveys did not discuss the SER in a natural environment and the effects of vocal burst features.

Page 26:
Speech Communication 154 (2023) 102974  26 A. Hashem et al.  El Ayadi, M., Kamel, M.S., Karray, F., 2011. Survey on speech emotion recognition: Features, classification schemes, and databases. Pattern Recognit. 44 (3), 572–587. Elbarougy, R., 2019. Extracting a discriminative acoustic features from voiced segments for improving speech emotion recognition accuracy. Int. J. Adv. Res. Comput. Sci. Electron. Eng. 8 (9), 39–44. Engberg, I.S., Hansen, A.V., Andersen, O., Dalsgaard, P., 1997. Design, recording and verification of a danish emotional speech database. In: Fifth European Conference on Speech Communication and Technology. Eskimez,   S.E.,   Duan,   Z.,   Heinzelman,   W.,   2018.   Unsupervised   learning   approach to   feature   analysis   for   automatic   speech   emotion   recognition.   In:   2018   IEEE International   Conference   on   Acoustics,   Speech   and   Signal   Processing   (ICASSP). IEEE, pp. 5099–5103. Etienne, C., Fidanza, G., Petrovskii, A., Devillers, L., Schmauch, B., 2018. Cnn+ lstm architecture for speech emotion recognition with data augmentation. arXiv preprint arXiv:1802.05630. Eyben,   F.,   Scherer,   K.R.,   Schuller,   B.W.,   Sundberg,   J.,   André,   E.,   Busso,   C.,   Dev- illers,   L.Y.,   Epps,   J.,   Laukka,   P.,   Narayanan,   S.S.,   et   al.,   2015.   The   geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans. Affect. Comput. 7 (2), 190–202. Eyben, F., Wöllmer, M., Schuller, B., 2010. Opensmile: the munich versatile and fast open-source audio feature extractor. In: Proceedings of the 18th ACM International Conference on Multimedia. pp. 1459–1462. Fahad, M.S., Ranjan, A., Yadav, J., Deepak, A., 2021. A survey of speech emotion recognition in natural environment. Digit. Signal Process. 110, 102951. Falahzadeh, M.R., Farokhi, F., Harimi, A., Sabbaghi-Nadooshan, R., 2022. Deep convo- lutional neural network and gray wolf optimization algorithm for speech emotion recognition. Circuits Systems Signal Process. 1–44. Fan, W., Xu, X., Xing, X., Chen, W., Huang, D., 2021. LSSED: a large-scale dataset and   benchmark   for   speech   emotion   recognition.   In:   ICASSP   2021-2021   IEEE International   Conference   on   Acoustics,   Speech   and   Signal   Processing   (ICASSP). IEEE, pp. 641–645. Fayek, H.M., Lech, M., Cavedon, L., 2015. Towards real-time speech emotion recogni- tion using deep neural networks. In: 2015 9th International Conference on Signal Processing and Communication Systems (ICSPCS). IEEE, pp. 1–5. Fayek, H.M., Lech, M., Cavedon, L., 2017. Evaluating deep learning architectures for speech emotion recognition. Neural Netw. 92, 60–68. Fernandes, J., Teixeira, F., Guedes, V., Junior, A., Teixeira, J.P., 2018. Harmonic to noise ratio measurement-selection of window and length. Procedia Comput. Sci. 138, 280–285. Fleischer,   M.,   Pinkert,   S.,   Mattheus,   W.,   Mainka,   A.,   Mürbe,   D.,   2015.   Formant frequencies and bandwidths of the vocal tract transfer function are affected by the mechanical impedance of the vocal tract wall. Biomech. Model. Mechanobiol. 14, 719–733. Freitag, M., Amiriparian, S., Pugachevskiy, S., Cummins, N., Schuller, B., 2017. audeep: Unsupervised learning of representations from audio with deep recurrent neural networks. J. Mach. Learn. Res. 18 (1), 6340–6344. Gangamohan, P., Kadiri, S.R., Yegnanarayana, B., 2016. Analysis of emotional speech— A   review.   In:   Toward   Robotic   Socially   Believable   Behaving   Systems-Volume   I: Modeling Emotions. Springer, pp. 205–238. Gao, Y., Li, B., Wang, N., Zhu, T., 2017. Speech emotion recognition using local and global features. In: Brain Informatics: International Conference, BI 2017, Beijing, China, November 16-18, 2017, Proceedings. Springer, pp. 3–13. Gilke, M., Kachare, P., Kothalikar, R., Rodrigues, V.P., Pednekar, M., 2012. MFCC-based vocal emotion recognition using ANN. In: International Conference on Electronics Engineering and Informatics (ICEEI 2012) IPCSIT, Vol. 49. Grandjean, D., Sander, D., Scherer, K.R., 2008. Conscious emotional experience emerges as a function of multilevel, appraisal-driven response synchronization. Conscious. Cognit. 17 (2), 484–495. Grimm, M., Kroschel, K., Narayanan, S., 2008. The vera am Mittag German audio-visual emotional speech database. In: 2008 IEEE International Conference on Multimedia and Expo. IEEE, pp. 865–868. Gunawan, T.S., Alghifari, M.F., Morshidi, M.A., Kartiwi, M., 2018. A review on emotion recognition algorithms using speech analysis. Indones. J. Electr. Eng. Inform. (IJEEI) 6 (1), 12–20. Gunes,   H.,   Pantic,   M.,   2010.   Automatic,   dimensional   and   continuous   emotion recognition. Int. J. Synth. Emot. (IJSE) 1 (1), 68–99. Guo,   L.,   Wang,   L.,   Dang,   J.,   Chng,   E.S.,   Nakagawa,   S.,   2022.   Learning   affective representations based on magnitude and dynamic relative phase information for speech emotion recognition. Speech Commun. 136, 118–127. Guo, L., Wang, L., Dang, J., Zhang, L., Guan, H., 2018. A feature fusion method based on extreme learning machine for speech emotion recognition. In: 2018 IEEE International   Conference   on   Acoustics,   Speech   and   Signal   Processing   (ICASSP). IEEE, pp. 2666–2670. Gupta,   S.,   Fahad,   M.S.,   Deepak,   A.,   2020.   Pitch-synchronous   single   frequency   fil- tering spectrogram for speech emotion recognition. Multimedia Tools Appl. 79, 23347–23365. Hamid, O.K., 2018. Frame blocking and windowing speech signal. J. Inf. Commun. Intell. Syst. (JICIS) 4 (5), 87–94. Han, W., Jiang, T., Li, Y., Schuller, B., Ruan, H., 2020. Ordinal learning for emotion recognition in customer service calls. In: ICASSP 2020-2020 IEEE International Conference   on   Acoustics,   Speech   and   Signal   Processing   (ICASSP).   IEEE,   pp. 6494–6498. Han, K., Yu, D., Tashev, I., 2014. Speech emotion recognition using deep neural network and extreme learning machine. In: Interspeech 2014. Haq, S., Jackson, P.J., 2011. Multimodal emotion recognition. In: Machine Audition: Principles, Algorithms and Systems. IGI Global, pp. 398–423. He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recog- nition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778. Heinzel, G., Rüdiger, A., Schilling, R., 2002. Spectrum and spectral density estimation by the discrete Fourier transform (DFT), including a comprehensive list of window functions and some new at-top windows. Henríquez, P., Alonso, J.B., Ferrer, M.A., Travieso, C.M., Orozco-Arroyave, J.R., 2014. Nonlinear dynamics characterization of emotional speech. Neurocomputing 132, 126–135. Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Comput. 9 (8), 1735–1780. Hsu,   J.-H.,   Su,   M.-H.,   Wu,   C.-H.,   Chen,   Y.-H.,   2021.   Speech   emotion   recognition considering nonverbal vocalization in affective conversations. IEEE/ACM Trans. Audio Speech Lang. Process. 29, 1675–1686. Huang, K.-Y., Wu, C.-H., Hong, Q.-B., Su, M.-H., Chen, Y.-H., 2019. Speech emotion recognition using deep neural network considering verbal and nonverbal speech sounds. In: ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 5866–5870. Huang, G.-B., Zhu, Q.-Y., Siew, C.-K., 2006. Extreme learning machine: theory and applications. Neurocomputing 70 (1–3), 489–501. Issa,   D.,   Demirci,   M.F.,   Yazici,   A.,   2020.   Speech   emotion   recognition   with   deep convolutional neural networks. Biomed. Signal Process. Control 59, 101894. Jahangir, R., Teh, Y.W., Hanif, F., Mujtaba, G., 2021. Deep learning approaches for speech emotion recognition: State of the art and research challenges. Multimedia Tools Appl. 80 (16), 23745–23812. Jain, M., Narayan, S., Balaji, P., Bhowmick, A., Muthu, R.K., et al., 2020. Speech emotion   recognition   using   support   vector   machine.   arXiv   preprint   arXiv:2002. 07590. Jalal, M.A., Milner, R., Hain, T., 2020. Empirical interpretation of speech emotion per- ception with attention based model for speech emotion recognition. In: Proceedings of Interspeech 2020. International Speech Communication Association (ISCA), pp. 4113–4117. Jiang, P., Fu, H., Tao, H., 2019. Speech emotion recognition using deep convolutional neural network and simple recurrent unit. Eng. Lett. 27 (4). Jiang, X., Paulmann, S., Robin, J., Pell, M.D., 2015. More than accuracy: Nonverbal dialects modulate the time course of vocal emotion recognition across cultures. J. Exp. Psychol.: Hum. Percept. Perform. 41 (3), 597. Jing, S., Mao, X., Chen, L., 2018. Prominence features: Effective emotional features for speech emotion recognition. Digit. Signal Process. 72, 216–231. Joshi,   H.,   Verma,   A.,   Mishra,   A.,   2020.   Classification   of   social   signals   using   deep LSTM-based recurrent neural networks. In: 2020 International Conference on Signal Processing and Communications (SPCOM). IEEE, pp. 1–5. Jovic,   A.,   Brkic,   K.,   Bogunovic,   N.,   2014.   An   overview   of   free   software   tools   for general data mining. In: 2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO). IEEE, pp. 1112–1117. Jovicic, S.T., Kasic, Z., Dordevic, M., Rajkovic, M., 2004. Serbian emotional speech database:   design,   processing   and   evaluation.   In:   9th   Conference   Speech   and Computer. Kaiser, J.F., 1990. On a simple algorithm to calculate the ‘energy’ of a signal. In: International Conference on Acoustics, Speech, and Signal Processing. IEEE, pp. 381–384. Karadoğan,   S.G.,   Larsen,   J.,   2012.   Combining   semantic   and   acoustic   features   for valence and arousal recognition in speech. In: 2012 3rd International Workshop on Cognitive Information Processing (CIP). IEEE, pp. 1–6. Keele, S., et al., 2007. Guidelines for Performing Systematic Literature Reviews in Software Engineering. Technical Report, Technical Report, Ver. 2.3 Ebse Technical Report, EBSE. Keesing, A., Koh, Y.S., Witbrock, M., 2021. Acoustic features and neural representations for categorical emotion recognition from speech. In: Interspeech. pp. 3415–3419. Khalil, R.A., Jones, E., Babar, M.I., Jan, T., Zafar, M.H., Alhussain, T., 2019. Speech emotion recognition using deep learning techniques: A review. IEEE Access 7, 117327–117345. Kim,   Y.,   Provost,   E.M.,   2013.   Emotion   classification   via   utterance-level   dynamics: A pattern-based approach to characterizing affective expressions. In: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, pp. 3677–3681. Koolagudi, S.G., Rao, K.S., 2012. Emotion recognition from speech: a review. Int. J. Speech Technol. 15 (2), 99–117. Kumar, A., Sharma, K., Sharma, A., 2022. Memor: A multimodal emotion recognition using affective biomarkers for smart prediction of emotional health for people analytics in smart industries. Image Vis. Comput. 123, 104483.

